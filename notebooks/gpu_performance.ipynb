{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU-Accelerated Boolean Function Analysis\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GabbyTab/boofun/blob/main/notebooks/gpu_performance.ipynb)\n",
    "\n",
    "This notebook demonstrates GPU acceleration for Boolean function analysis\n",
    "using CuPy.  **Run it on Google Colab with a GPU runtime** to see the\n",
    "speed-ups.\n",
    "\n",
    "> **Runtime → Change runtime type → T4 GPU**\n",
    "\n",
    "We benchmark:\n",
    "\n",
    "1. Walsh-Hadamard Transform (WHT)\n",
    "2. Influence computation\n",
    "3. Noise stability\n",
    "4. Spectral weight by degree\n",
    "\n",
    "and compare GPU (CuPy) vs CPU (NumPy) across increasing $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup — install boofun and CuPy\n",
    "!pip install --upgrade boofun -q\n",
    "!pip install cupy-cuda12x -q 2>/dev/null || echo \"CuPy install skipped (no CUDA)\"\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import boofun as bf\n",
    "from boofun.core.gpu import (\n",
    "    is_gpu_available,\n",
    "    is_gpu_enabled,\n",
    "    enable_gpu,\n",
    "    get_gpu_info,\n",
    "    gpu_walsh_hadamard,\n",
    "    gpu_influences,\n",
    "    gpu_noise_stability,\n",
    "    gpu_spectral_weight_by_degree,\n",
    "    GPUBooleanFunctionOps,\n",
    ")\n",
    "from boofun.core.optimizations import fast_walsh_hadamard\n",
    "\n",
    "print(f\"boofun version: {bf.__version__}\")\n",
    "print(f\"GPU available:  {is_gpu_available()}\")\n",
    "print(f\"GPU enabled:    {is_gpu_enabled()}\")\n",
    "if is_gpu_available():\n",
    "    info = get_gpu_info()\n",
    "    for d in info.get('devices', []):\n",
    "        print(f\"  Device: {d['name']} ({d['memory_gb']:.1f} GB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Benchmarking the Walsh-Hadamard Transform\n",
    "\n",
    "The WHT is the core computation: it converts a truth table in $\\pm 1$\n",
    "form to Fourier coefficients in $O(n \\cdot 2^n)$ time.  For large $n$,\n",
    "this is where GPU parallelism helps most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_wht(n_values, n_trials=3):\n",
    "    \"\"\"Benchmark WHT on CPU vs GPU for various n.\"\"\"\n",
    "    cpu_times = []\n",
    "    gpu_times = []\n",
    "\n",
    "    for n in n_values:\n",
    "        size = 2 ** n\n",
    "        # Random ±1 function\n",
    "        values = np.random.choice([-1.0, 1.0], size=size)\n",
    "\n",
    "        # CPU benchmark\n",
    "        t_cpu = []\n",
    "        for _ in range(n_trials):\n",
    "            v = values.copy()\n",
    "            start = time.perf_counter()\n",
    "            fast_walsh_hadamard(v)\n",
    "            t_cpu.append(time.perf_counter() - start)\n",
    "        cpu_times.append(np.median(t_cpu))\n",
    "\n",
    "        # GPU benchmark (falls back to CPU if no GPU)\n",
    "        if is_gpu_enabled():\n",
    "            enable_gpu(True)\n",
    "            t_gpu = []\n",
    "            for _ in range(n_trials):\n",
    "                start = time.perf_counter()\n",
    "                gpu_walsh_hadamard(values.copy())\n",
    "                t_gpu.append(time.perf_counter() - start)\n",
    "            gpu_times.append(np.median(t_gpu))\n",
    "        else:\n",
    "            gpu_times.append(None)\n",
    "\n",
    "        print(f\"n={n:2d} | CPU: {cpu_times[-1]*1000:8.2f} ms\"\n",
    "              f\" | GPU: {gpu_times[-1]*1000 if gpu_times[-1] else 'N/A':>8} ms\")\n",
    "\n",
    "    return cpu_times, gpu_times\n",
    "\n",
    "n_values = list(range(8, 22))\n",
    "cpu_times, gpu_times = benchmark_wht(n_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "cpu_ms = [t * 1000 for t in cpu_times]\n",
    "ax1.semilogy(n_values, cpu_ms, 'bo-', label='CPU (NumPy)', markersize=6)\n",
    "if any(g is not None for g in gpu_times):\n",
    "    gpu_ms = [t * 1000 if t else None for t in gpu_times]\n",
    "    ax1.semilogy(n_values, gpu_ms, 'rs-', label='GPU (CuPy)', markersize=6)\n",
    "ax1.set_xlabel('n (variables)')\n",
    "ax1.set_ylabel('Time (ms, log scale)')\n",
    "ax1.set_title('Walsh-Hadamard Transform: CPU vs GPU')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Speedup plot\n",
    "if any(g is not None for g in gpu_times):\n",
    "    speedups = [c / g if g else 0 for c, g in zip(cpu_times, gpu_times)]\n",
    "    ax2.bar(n_values, speedups, color='green', alpha=0.7)\n",
    "    ax2.axhline(y=1.0, color='red', linestyle='--', label='Break-even')\n",
    "    ax2.set_xlabel('n (variables)')\n",
    "    ax2.set_ylabel('Speedup (CPU time / GPU time)')\n",
    "    ax2.set_title('GPU Speedup Factor')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, 'No GPU available\\n(run on Colab with GPU runtime)',\n",
    "             ha='center', va='center', transform=ax2.transAxes, fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPU-Accelerated Spectral Analysis\n",
    "\n",
    "The `GPUBooleanFunctionOps` class wraps a truth table and provides\n",
    "GPU-accelerated Fourier analysis, influences, noise stability, and\n",
    "spectral weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build majority(11) and compare CPU vs GPU analysis\n",
    "n = 11\n",
    "f = bf.majority(n)\n",
    "tt = np.array(f.get_representation('truth_table'), dtype=float)\n",
    "\n",
    "# CPU path\n",
    "start = time.perf_counter()\n",
    "fourier_cpu = f.fourier()\n",
    "inf_cpu = np.array([f.influence(i) for i in range(n)])\n",
    "stab_cpu = f.noise_stability(0.5)\n",
    "t_cpu = time.perf_counter() - start\n",
    "\n",
    "# GPU path\n",
    "ops = GPUBooleanFunctionOps(tt)\n",
    "start = time.perf_counter()\n",
    "fourier_gpu = ops.fourier()\n",
    "inf_gpu = ops.influences()\n",
    "stab_gpu = ops.noise_stability(0.5)\n",
    "weights_gpu = ops.spectral_weights()\n",
    "t_gpu = time.perf_counter() - start\n",
    "\n",
    "print(f\"Majority({n}) analysis:\")\n",
    "print(f\"  CPU time: {t_cpu*1000:.1f} ms\")\n",
    "print(f\"  GPU time: {t_gpu*1000:.1f} ms\")\n",
    "print(f\"  Speedup:  {t_cpu/t_gpu:.1f}x\" if t_gpu > 0 else \"\")\n",
    "print(f\"\\n  Noise stability (rho=0.5): {stab_gpu:.6f}\")\n",
    "print(f\"  Total influence: {ops.total_influence():.4f}\")\n",
    "print(f\"  Spectral weights by degree: {[f'{w:.4f}' for w in weights_gpu]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance Tips Summary\n",
    "\n",
    "| Technique | When to use | How |\n",
    "|---|---|---|\n",
    "| **NumPy vectorisation** | Always (free speed) | Use array operations, avoid Python loops |\n",
    "| **In-memory truth tables** | n ≤ 20 | Default — `bf.create()`, `.fourier()` |\n",
    "| **Numba JIT** | Tight inner loops | `pip install boofun[performance]` |\n",
    "| **CuPy GPU** | n > 14 for WHT; large batch operations | `pip install cupy-cuda12x` |\n",
    "| **Batch processing** | Evaluating many inputs | `boofun.core.batch_processing` |\n",
    "\n",
    "See `docs/guides/performance.md` for detailed guidance."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
