{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Fractional Pseudorandom Generator\n",
    "\n",
    "**Paper**: Chattopadhyay, Hatami, Lovett, Tal. [Pseudorandom Generators from the Second Fourier Level and Applications to AC0 with Parity Gates](https://doi.org/10.4230/LIPIcs.ITCS.2019.22). *ITCS 2019*.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook walks through the construction of a **fractional pseudorandom generator** from the paper. The construction has several moving parts, so it helps to see each one happen concretely. At the end, we use the construction as an excuse to poke at Conjecture 3 on small instances.\n",
    "\n",
    "### What the paper does\n",
    "\n",
    "A PRG for a function class $F$ produces samples that no $f \\in F$ can distinguish from truly random inputs. The paper builds this in two stages:\n",
    "\n",
    "1. A **fractional PRG** -- samples in $[-1,1]^n$ that fool $f$'s multilinear extension, using only $\\ell \\ll n$ random values.\n",
    "2. A **polarizing random walk** (Theorem 7, from [CHHL'18]) converts the fractional output to Boolean $\\{-1,+1\\}^n$.\n",
    "\n",
    "We implement stage 1. The paper's target application is AC0[$\\oplus$] circuits, but the construction works for any restriction-closed class with bounded $L_{1,2}$. We test it on degree-$d$ polynomials over $\\mathbb{F}_2$, which is the class the paper focuses on.\n",
    "\n",
    "### Roadmap\n",
    "\n",
    "1. Background: function classes, Fourier tails, the $L_{1,2}$ condition\n",
    "2. Test functions: random degree-2 $\\mathbb{F}_2$ polynomials\n",
    "3. Theorem 9: small-variance Gaussians fool bounded-tail functions\n",
    "4. Dimension reduction via balanced codes\n",
    "5. The complete fractional PRG\n",
    "6. Poking at Conjecture 3"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!pip install --upgrade \"boofun[performance]\" -q\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import boofun as bf\n",
    "from boofun.analysis.fourier import fourier_level_lp_norm\n",
    "from boofun.analysis.gf2 import gf2_degree\n",
    "from scipy.stats import norm\n",
    "from itertools import combinations\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"boofun version: {bf.__version__}\")\n",
    "\n",
    "# Check acceleration backends\n",
    "from boofun.core.numba_optimizations import is_numba_available\n",
    "\n",
    "print(f\"Numba available: {is_numba_available()}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "\n",
    "### Function classes\n",
    "\n",
    "The paper works with a class $F$ of Boolean functions that is *closed under restrictions*: if $f \\in F$ and you fix some variables to constants, the restricted function is still in $F$. This property is needed because the PRG construction progressively restricts variables (via the polarizing walk in Theorem 7).\n",
    "\n",
    "The class we use: **degree-$d$ polynomials over $\\mathbb{F}_2$**, denoted $\\text{Poly}_{n,d}$. A member is $f(x) = (-1)^{p(x)}$ where $p : \\mathbb{F}_2^n \\to \\mathbb{F}_2$ is a polynomial of degree $\\leq d$. For example, with $d = 2$ on 4 variables:\n",
    "\n",
    "$$p(x) = x_0 x_1 \\oplus x_2 x_3 \\oplus x_0 \\qquad (\\text{degree 2})$$\n",
    "\n",
    "Fixing $x_0 = 1$ gives $p(x) = x_1 \\oplus x_2 x_3 \\oplus 1$, still degree $\\leq 2$. The class is restriction-closed.\n",
    "\n",
    "### Fourier tails\n",
    "\n",
    "Every $f: \\{-1,+1\\}^n \\to \\{-1,+1\\}$ has a unique multilinear expansion $\\tilde{f}(x) = \\sum_{S \\subseteq [n]} \\hat{f}(S) \\prod_{i \\in S} x_i$. The **level-$k$ Fourier tail** sums the absolute values of coefficients at degree $k$:\n",
    "\n",
    "$$L_{1,k}(f) = \\sum_{|S|=k} |\\hat{f}(S)|$$\n",
    "\n",
    "The paper's main result (Theorem 2/6): if $F$ is restriction-closed and $L_{1,2}(F) \\leq t$, then an explicit PRG exists with seed length $O((t/\\varepsilon)^{2+o(1)} \\cdot \\text{polylog}(n))$. Only the second level matters.\n",
    "\n",
    "### What is $t$ for our class?\n",
    "\n",
    "For $\\text{Poly}_{n,d}$, it is known from [CHHL'18] that $L_{1,2}(\\text{Poly}_{n,d}) \\leq 4 \\cdot 2^{6d}$ (exponential in $d$, but independent of $n$). **Conjecture 3** says the truth is $O(d^2)$. If true, combined with Theorem 2, this would give PRGs for AC0[$\\oplus$] -- a well-known open problem.\n",
    "\n",
    "In the paper, $t$ is an analytical worst-case bound over the entire class. Here, we compute $L_{1,2}$ exactly for specific instances. The PRG we build would fool any function with $L_{1,2} \\leq t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Functions: Random Degree-2 $\\mathbb{F}_2$ Polynomials\n",
    "\n",
    "We generate random quadratic polynomials over $\\mathbb{F}_2$ on $n = 16$ variables. Each polynomial is a random subset of the $\\binom{16}{2} = 120$ quadratic monomials plus the 16 linear monomials. The truth table has $2^{16} = 65{,}536$ entries -- small enough for exact Fourier analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n = 16\n",
    "d = 2\n",
    "\n",
    "\n",
    "def random_f2_poly(n, d, rng):\n",
    "    \"\"\"Generate a random degree-d polynomial over F2 on n variables.\"\"\"\n",
    "    monomials = []\n",
    "    for deg in range(1, d + 1):\n",
    "        for combo in combinations(range(n), deg):\n",
    "            if rng.random() < 0.5:\n",
    "                monomials.append(set(combo))\n",
    "    if not monomials:\n",
    "        monomials = [{0, 1}]  # avoid the constant function\n",
    "    return bf.f2_polynomial(n, monomials)\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "test_fns = {f\"quad_{i}\": random_f2_poly(n, d, rng) for i in range(5)}\n",
    "\n",
    "# Compute Fourier tails\n",
    "print(f\"Random degree-{d} F2 polynomials on n = {n} variables\")\n",
    "print(f\"Known bound (CHHL'18): L_{{1,2}}(Poly_{{n,{d}}}) <= 4 * 2^(6*{d}) = {4 * 2**(6*d)}\")\n",
    "print(f\"Conjecture 3:          L_{{1,2}} = O(d^2) = O({d**2})\")\n",
    "print()\n",
    "\n",
    "header = f\"{'Function':<12}{'GF2 deg':<10}\" + \"\".join(\n",
    "    f\"{'L_{1,' + str(k) + '}':<12}\" for k in range(5)\n",
    ")\n",
    "print(header)\n",
    "print(\"-\" * 82)\n",
    "\n",
    "l12_values = {}\n",
    "for name, func in test_fns.items():\n",
    "    gf2_d = gf2_degree(func)\n",
    "    row = f\"{name:<12}{gf2_d:<10}\"\n",
    "    for k in range(5):\n",
    "        val = fourier_level_lp_norm(func, k)\n",
    "        row += f\"{val:<12.4f}\"\n",
    "        if k == 2:\n",
    "            l12_values[name] = val\n",
    "    print(row)\n",
    "\n",
    "t = max(l12_values.values())\n",
    "print(f\"\\nEmpirical t = max L_{{1,2}} across our instances: {t:.4f}\")\n",
    "print(f\"Known bound for d={d}: {4 * 2**(6*d)}  (exponential, from [CHHL'18])\")\n",
    "print(f\"Conjectured bound for d={d}: O({d**2})  (Conjecture 3)\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Multilinear extension evaluator.\n",
    "#\n",
    "# Builds all 2^n monomials prod_{j in S} x_j incrementally:\n",
    "# at each variable i, doubles the range by multiplying existing\n",
    "# entries (subsets not containing i) by x_i (subsets containing i).\n",
    "# Then the dot product with Fourier coefficients gives f_tilde(x).\n",
    "\n",
    "\n",
    "def eval_multilinear_batch(fourier_coeffs, n_vars, X, batch_size=100):\n",
    "    \"\"\"Evaluate multilinear extension at rows of X.\"\"\"\n",
    "    m = X.shape[0]\n",
    "    size = len(fourier_coeffs)\n",
    "    results = np.zeros(m)\n",
    "    for start in range(0, m, batch_size):\n",
    "        end = min(start + batch_size, m)\n",
    "        batch = X[start:end]\n",
    "        b = batch.shape[0]\n",
    "        vals = np.zeros((b, size))\n",
    "        vals[:, 0] = 1.0  # empty product\n",
    "        for i in range(n_vars):\n",
    "            step = 1 << i\n",
    "            vals[:, step : 2 * step] = vals[:, :step] * batch[:, i : i + 1]\n",
    "        results[start:end] = vals @ fourier_coeffs\n",
    "    return results\n",
    "\n",
    "\n",
    "# --- Sanity check ---\n",
    "# f_tilde(0) should equal f_hat(empty) = E[f] for every test function.\n",
    "print(\"Sanity check: f_tilde(0) = f_hat(empty) = E[f]\")\n",
    "for name, func in test_fns.items():\n",
    "    fourier = func.fourier()\n",
    "    f_hat_0 = fourier[0]\n",
    "    f_tilde_0 = eval_multilinear_batch(fourier, n, np.zeros((1, n)))[0]\n",
    "    assert abs(f_tilde_0 - f_hat_0) < 1e-12, f\"{name}: mismatch!\"\n",
    "    print(f\"  {name}: f_hat(empty) = {f_hat_0:+.6f},  f_tilde(0) = {f_tilde_0:+.6f}  OK\")\n",
    "\n",
    "# Also verify Parseval: sum f_hat(S)^2 = 1 for +/-1 functions.\n",
    "for name, func in test_fns.items():\n",
    "    fourier = func.fourier()\n",
    "    parseval = np.sum(fourier**2)\n",
    "    assert abs(parseval - 1.0) < 1e-10, f\"{name}: Parseval violated! sum = {parseval}\"\n",
    "print(f\"\\nParseval check passed for all {len(test_fns)} functions.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Theorem 9: Gaussians Fool Bounded-Tail Functions\n",
    "\n",
    "The construction relies on this result (Raz-Tal, restated as Theorem 9 / Theorem 21 in the paper):\n",
    "\n",
    "> **Theorem 9.** Let $Z \\in \\mathbb{R}^n$ be a zero-mean multivariate Gaussian with:\n",
    "> - $\\text{Var}[Z_i] \\leq \\sigma^2$ for all $i$\n",
    "> - $|\\text{Cov}[Z_i, Z_j]| \\leq \\delta$ for all $i \\neq j$\n",
    ">\n",
    "> If $L_{1,2}(F) \\leq t$, then for any $f \\in F$:\n",
    "> $$|\\mathbb{E}[\\tilde{f}(\\text{trnc}(Z))] - \\tilde{f}(\\mathbf{0})| \\leq 4\\delta \\cdot t + 4n \\cdot e^{-1/(8\\sigma^2)}$$\n",
    "\n",
    "where $\\text{trnc}$ clips each coordinate to $[-1, 1]$, and $\\tilde{f}(\\mathbf{0}) = \\hat{f}(\\emptyset) = \\mathbb{E}[f]$.\n",
    "\n",
    "*Why only level 2?* Expanding $\\mathbb{E}[\\tilde{f}(Z)]$ by Fourier level: the level-0 term gives $\\tilde{f}(\\mathbf{0})$ (the target), level-1 terms vanish (zero mean), and level-2 terms contribute $\\sum_{|S|=2} |\\hat{f}(S)| \\cdot |\\text{Cov}(Z_i, Z_j)| \\leq \\delta \\cdot L_{1,2}(f)$. Higher levels are negligible when $\\sigma$ is small.\n",
    "\n",
    "The construction sets $\\sigma^2 = p = 1/(8\\ln(n/\\delta))$, which makes the second term equal to $4\\delta$ (since $e^{-\\ln(n/\\delta)} = \\delta/n$). The full bound becomes $4\\delta(t + 1)$. Setting $\\delta = \\varepsilon/(4(t+1))$ gives error $\\leq \\varepsilon$.\n",
    "\n",
    "**Note on scale**: at $n = 16$ with small $t$, the constant factors in the bound matter. The bound is designed for large $n$ and moderate $t$. We verify the construction works empirically.\n",
    "\n",
    "As a baseline, we first try $n$ independent Gaussians (trivially $\\delta = 0$, but no randomness savings)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Parameters from the bound: 4*delta*(t+1) <= eps\n",
    "# => delta = eps / (4*(t+1))\n",
    "eps = 0.3\n",
    "delta = eps / (4 * (t + 1))\n",
    "p_var = 1.0 / (8 * np.log(n / delta))  # = 1/(8 ln(n/delta))\n",
    "sigma = np.sqrt(p_var)\n",
    "\n",
    "# Full Theorem 21 bound: 4*delta*t + 4*n*exp(-1/(8*sigma^2))\n",
    "# With sigma^2 = 1/(8 ln(n/delta)), the exp term equals 4*delta.\n",
    "trunc_term = 4 * n * np.exp(-1 / (8 * p_var))\n",
    "full_bound = 4 * delta * t + trunc_term\n",
    "\n",
    "print(\"Theorem 9 parameters:\")\n",
    "print(f\"  n = {n},  t = {t:.4f},  eps = {eps}\")\n",
    "print(f\"  delta = eps/(4(t+1)) = {delta:.6f}\")\n",
    "print(f\"  p = 1/(8 ln(n/delta)) = {p_var:.6f},  sigma = {sigma:.6f}\")\n",
    "print(f\"  Bound term 1: 4*delta*t = {4*delta*t:.4f}\")\n",
    "print(f\"  Bound term 2: 4n*exp(-1/(8p)) = {trunc_term:.4f}  (= 4*delta by construction)\")\n",
    "print(f\"  Full Theorem 21 bound: {full_bound:.4f}\")\n",
    "print()\n",
    "\n",
    "# --- Naive baseline: n independent Gaussians ---\n",
    "n_samples = 2000\n",
    "naive_results = {}\n",
    "print(f\"Naive baseline ({n_samples} samples, {n} independent Gaussians each):\")\n",
    "for name, func in test_fns.items():\n",
    "    fourier = func.fourier()\n",
    "    E_f = fourier[0]\n",
    "    Z = np.random.randn(n_samples, n) * sigma\n",
    "    X = np.clip(Z, -1, 1)\n",
    "    vals = eval_multilinear_batch(fourier, n, X)\n",
    "    err = abs(np.mean(vals) - E_f)\n",
    "    naive_results[name] = {\"E_f\": E_f, \"mean\": np.mean(vals), \"error\": err}\n",
    "    print(f\"  {name}: E[f] = {E_f:+.4f}, mean = {np.mean(vals):+.4f}, |error| = {err:.6f}\")\n",
    "\n",
    "print(f\"\\nWith independent coordinates, delta = 0 and the fooling error is 0 in theory.\")\n",
    "print(f\"The small empirical errors above are finite-sample noise.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimension Reduction via Balanced Codes\n",
    "\n",
    "Instead of $n$ independent Gaussians, the paper generates $n$ *correlated* values from only $\\ell$ independent Gaussians.\n",
    "\n",
    "**Construction** (Paper, Section 2, Step I):\n",
    "1. Choose $n$ distinct codewords $c_1, \\ldots, c_n \\in \\{0,1\\}^\\ell$ from a $\\delta$-balanced code\n",
    "2. Build $A \\in \\mathbb{R}^{n \\times \\ell}$: $\\; A_{i,j} = \\sqrt{p/\\ell} \\cdot (-1)^{c_{i,j}}$\n",
    "3. Sample $Y \\sim N(0, I_\\ell)$ -- only $\\ell$ random values\n",
    "4. Set $Z = AY$\n",
    "\n",
    "Each row of $A$ has squared norm $p$, so $\\text{Var}[Z_i] = p$ exactly. The covariance between $Z_i$ and $Z_j$ is:\n",
    "\n",
    "$$\\text{Cov}(Z_i, Z_j) = p \\cdot \\left(1 - \\frac{2\\, d_H(c_i, c_j)}{\\ell}\\right)$$\n",
    "\n",
    "where $d_H$ is the Hamming distance. Pairs with $d_H \\approx \\ell/2$ have near-zero covariance.\n",
    "\n",
    "The required code length is $\\ell = O(\\log n / \\delta^{2+o(1)})$ using Ta-Shma's explicit construction. At $n = 16$ the theoretical $\\ell$ exceeds $n$, so there's no dimension saving at this scale. We use $\\ell = 10$ to show the mechanics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Dimension reduction: n -> ell\n",
    "ell_theory = int(np.ceil((max(t, 1) / eps) ** 2 * np.log2(n) * 10))\n",
    "ell = 10  # 16 -> 10\n",
    "\n",
    "print(f\"Dimension reduction: {n} -> {ell}\")\n",
    "print(f\"  Theoretical ell for delta-balanced code ~ {ell_theory}\")\n",
    "print(f\"  At this scale ell > n; savings appear for large n.\")\n",
    "print()\n",
    "\n",
    "# --- Balanced code ---\n",
    "# The paper uses explicit small-biased spaces (Ta-Shma 2017).\n",
    "# Here: random codewords, ensuring all are distinct.\n",
    "# Find a seed that gives all-unique codewords with good separation\n",
    "for _seed in range(100):\n",
    "    code_rng = np.random.default_rng(seed=_seed)\n",
    "    codewords = code_rng.integers(0, 2, size=(n, ell))\n",
    "    if len(set(map(tuple, codewords))) == n:\n",
    "        _dists = [np.sum(codewords[i] != codewords[j]) for i, j in combinations(range(n), 2)]\n",
    "        if min(_dists) >= 2:\n",
    "            break\n",
    "assert len(set(map(tuple, codewords))) == n, \"Duplicate codewords!\"\n",
    "\n",
    "# Report code quality\n",
    "hamming_dists = []\n",
    "for i, j in combinations(range(n), 2):\n",
    "    hamming_dists.append(np.sum(codewords[i] != codewords[j]))\n",
    "print(f\"Code quality ({n} codewords, length {ell}):\")\n",
    "print(f\"  Pairwise Hamming distances: min={min(hamming_dists)}, max={max(hamming_dists)}, mean={np.mean(hamming_dists):.1f}\")\n",
    "print(f\"  Ideal: all distances = {ell//2} (half the length)\")\n",
    "print()\n",
    "\n",
    "# Matrix A: A_{i,j} = sqrt(p / ell) * (-1)^{c_{i,j}}\n",
    "scale = np.sqrt(p_var / ell)\n",
    "A = scale * (1 - 2 * codewords.astype(float))\n",
    "\n",
    "# Covariance structure\n",
    "cov_Z = A @ A.T\n",
    "diag = np.diag(cov_Z)\n",
    "off_diag = cov_Z[np.triu_indices(n, k=1)]\n",
    "effective_delta = np.max(np.abs(off_diag))\n",
    "\n",
    "print(f\"Covariance structure of Z = AY:\")\n",
    "print(f\"  Var[Z_i] = {diag[0]:.6f}  (= p, by construction)\")\n",
    "print(f\"  max |Cov[Z_i, Z_j]| = {effective_delta:.6f}  (effective delta)\")\n",
    "print(f\"  mean |Cov[Z_i, Z_j]| = {np.mean(np.abs(off_diag)):.6f}\")\n",
    "# Full bound using this code's effective delta:\n",
    "# 4*delta_eff*t + 4*delta_eff (truncation term scales with delta_eff too)\n",
    "eff_trunc = 4 * n * np.exp(-1 / (8 * p_var))\n",
    "eff_bound = 4 * effective_delta * t + eff_trunc\n",
    "print(f\"\\nFooling bound with this code: 4*delta_eff*t + trunc = {4*effective_delta*t:.4f} + {eff_trunc:.4f} = {eff_bound:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 4.5))\n",
    "\n",
    "im0 = axes[0].imshow(A, cmap=\"RdBu\", aspect=\"auto\")\n",
    "axes[0].set_xlabel(f\"Seed dimension ($\\ell$ = {ell})\")\n",
    "axes[0].set_ylabel(f\"Output dimension (n = {n})\")\n",
    "axes[0].set_title(f\"Matrix $A$ ({n} x {ell})\")\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "im1 = axes[1].imshow(cov_Z, cmap=\"RdBu\", vmin=-p_var, vmax=p_var)\n",
    "axes[1].set_title(f\"Covariance $AA^T$ ({n} x {n})\")\n",
    "plt.colorbar(im1, ax=axes[1], label=\"Cov\")\n",
    "\n",
    "axes[2].hist(off_diag, bins=25, color=\"steelblue\", edgecolor=\"white\", alpha=0.8)\n",
    "axes[2].axvline(x=0, color=\"black\", linewidth=0.8)\n",
    "axes[2].axvline(x=effective_delta, color=\"red\", linestyle=\"--\", label=f\"max |Cov| = {effective_delta:.4f}\")\n",
    "axes[2].axvline(x=-effective_delta, color=\"red\", linestyle=\"--\")\n",
    "axes[2].set_xlabel(\"Cov($Z_i$, $Z_j$)\")\n",
    "axes[2].set_ylabel(\"Count\")\n",
    "axes[2].set_title(\"Off-diagonal covariances\")\n",
    "axes[2].legend(fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Complete Fractional PRG\n",
    "\n",
    "**Discretization** (Paper, Lemma 10-11): each of the $\\ell$ continuous Gaussians is approximated by a discrete random variable using $k$ bits (Kane's construction: $k = O(\\log(1/\\lambda))$).\n",
    "\n",
    "The full pipeline:\n",
    "\n",
    "$$s \\text{ bits} \\;\\xrightarrow{\\text{quantize}}\\; Y \\in \\mathbb{R}^\\ell \\;\\xrightarrow{\\;Z = AY\\;}\\; Z \\in \\mathbb{R}^n \\;\\xrightarrow{\\;\\text{trnc}\\;}\\; X \\in [-1,1]^n$$\n",
    "\n",
    "| Stage | Input | Output |\n",
    "|-------|-------|--------|\n",
    "| Quantize | $s = \\ell \\cdot k$ random bits | $Y \\in \\mathbb{R}^\\ell$ (approx. Gaussians) |\n",
    "| Dimension expansion | $Y$ | $Z = AY \\in \\mathbb{R}^n$ (correlated, small covariance) |\n",
    "| Truncation | $Z$ | $X = \\text{trnc}(Z) \\in [-1,1]^n$ (fractional PRG output) |\n",
    "\n",
    "We check: for each test function, $|\\mathbb{E}[\\tilde{f}(X)] - \\mathbb{E}[f]|$ should be bounded by the Theorem 21 bound."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# --- Discretization ---\n",
    "bits_per_coord = 8\n",
    "num_levels = 2**bits_per_coord\n",
    "total_seed_bits = ell * bits_per_coord\n",
    "\n",
    "# Quantized Gaussian: 256-level approximation of N(0,1)\n",
    "quantized_gaussian = norm.ppf((np.arange(num_levels) + 0.5) / num_levels)\n",
    "\n",
    "print(f\"Discretization: {bits_per_coord} bits/coord, {num_levels} levels\")\n",
    "print(f\"Total seed: {ell} x {bits_per_coord} = {total_seed_bits} bits\")\n",
    "print()\n",
    "\n",
    "# === Complete Fractional PRG ===\n",
    "n_trials = 2000\n",
    "theoretical_bound = 4 * effective_delta * t + eff_trunc\n",
    "\n",
    "print(f\"Verification ({n_trials} trials)\")\n",
    "print(f\"  Seed: {total_seed_bits} bits -> {ell} Gaussians -> {n} fractional values\")\n",
    "print(f\"  Theoretical bound (Thm 21): {theoretical_bound:.4f}\")\n",
    "print()\n",
    "\n",
    "prg_results = {}\n",
    "for name, func in test_fns.items():\n",
    "    fourier = func.fourier()\n",
    "    E_f = fourier[0]\n",
    "\n",
    "    seeds = np.random.randint(0, num_levels, size=(n_trials, ell))\n",
    "    Y_batch = quantized_gaussian[seeds]\n",
    "    Z_batch = Y_batch @ A.T\n",
    "    X_batch = np.clip(Z_batch, -1, 1)\n",
    "\n",
    "    vals = eval_multilinear_batch(fourier, n, X_batch)\n",
    "    prg_mean = np.mean(vals)\n",
    "    prg_err = abs(prg_mean - E_f)\n",
    "\n",
    "    prg_results[name] = {\"E_f\": E_f, \"mean\": prg_mean, \"error\": prg_err}\n",
    "\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    E[f] = {E_f:+.6f}\")\n",
    "    print(f\"    PRG mean  = {prg_mean:+.6f}  (error = {prg_err:.6f})\")\n",
    "    print(f\"    Naive mean = {naive_results[name]['mean']:+.6f}  (error = {naive_results[name]['error']:.6f})\")\n",
    "    print()\n",
    "\n",
    "# --- Comparison plot ---\n",
    "names = list(test_fns.keys())\n",
    "x_pos = np.arange(len(names))\n",
    "width = 0.25\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "naive_errs = [naive_results[nm][\"error\"] for nm in names]\n",
    "prg_errs = [prg_results[nm][\"error\"] for nm in names]\n",
    "\n",
    "axes[0].bar(x_pos - width / 2, naive_errs, width, label=f\"Naive ({n} independent vals)\", color=\"steelblue\")\n",
    "axes[0].bar(x_pos + width / 2, prg_errs, width, label=f\"PRG ({ell} correlated vals)\", color=\"orange\")\n",
    "axes[0].axhline(\n",
    "    y=theoretical_bound,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"$4\\\\delta_{{eff}} \\\\cdot t$ = {theoretical_bound:.4f}\",\n",
    ")\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(names, fontsize=8, rotation=15)\n",
    "axes[0].set_ylabel(\"|mean - E[f]|\")\n",
    "axes[0].set_title(\"Fooling Error\")\n",
    "axes[0].legend(fontsize=7)\n",
    "axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Seed length scaling\n",
    "ns = [16, 64, 256, 1024, 4096, 16384, 65536]\n",
    "ell_values = [int(np.ceil((max(t, 1) / eps) ** 2 * np.log2(nn) * 5)) for nn in ns]\n",
    "axes[1].plot(ns, ns, \"k--\", label=\"n (naive)\", linewidth=2)\n",
    "axes[1].plot(ns, ell_values, \"o-\", color=\"orange\", label=r\"$\\ell$ (PRG)\", linewidth=2)\n",
    "crossover = next((nn for nn, ev in zip(ns, ell_values) if ev < nn), None)\n",
    "if crossover:\n",
    "    axes[1].axvline(\n",
    "        x=crossover, color=\"green\", linestyle=\":\", alpha=0.7, label=f\"Crossover ~ n={crossover}\"\n",
    "    )\n",
    "axes[1].set_xlabel(\"n (number of variables)\")\n",
    "axes[1].set_ylabel(\"Random values needed\")\n",
    "axes[1].set_title(r\"Seed dimension $\\ell$ vs $n$\")\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].set_yscale(\"log\")\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Poking at Conjecture 3\n",
    "\n",
    "**Conjecture 3** from the paper:\n",
    "\n",
    "> $L_{1,2}(\\text{Poly}_{n,d}) = O(d^2)$. That is, for any degree-$d$ polynomial $p : \\mathbb{F}_2^n \\to \\mathbb{F}_2$ and $f(x) = (-1)^{p(x)}$:\n",
    "> $$\\sum_{i<j} |\\hat{f}(\\{i,j\\})| = O(d^2)$$\n",
    "\n",
    "This is unproven. The known bound is $4 \\cdot 2^{6d}$ (exponential, from [CHHL'18]). If the conjecture holds, it gives PRGs for AC0[$\\oplus$] via the construction above.\n",
    "\n",
    "We compute $L_{1,2}$ for many random instances and see what the numbers look like. Two experiments:\n",
    "\n",
    "1. $L_{1,2}$ vs $d$ at fixed $n = 12$: does $L_{1,2}$ grow with $d$?\n",
    "2. $L_{1,2}$ vs $n$ at fixed $d = 2$: is $L_{1,2}$ independent of $n$?\n",
    "\n",
    "The conjecture says: polynomial growth in $d$, no dependence on $n$.\n",
    "\n",
    "These are small instances, so take the results as observations, not evidence."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === Experiment 1: L_{1,2} vs degree d (fixed n = 12) ===\n",
    "# Higher degrees have many monomials (C(n,d) grows fast), so we keep n moderate.\n",
    "n_conj = 12\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "n_per_degree = 120\n",
    "conj_rng = np.random.default_rng(seed=123)\n",
    "\n",
    "results_by_degree = {dv: [] for dv in degrees}\n",
    "\n",
    "for dv in degrees:\n",
    "    for _ in range(n_per_degree):\n",
    "        monomials = []\n",
    "        for deg in range(1, dv + 1):\n",
    "            for combo in combinations(range(n_conj), deg):\n",
    "                if conj_rng.random() < 0.3:\n",
    "                    monomials.append(set(combo))\n",
    "        if not monomials:\n",
    "            monomials = [{0}]\n",
    "        f = bf.f2_polynomial(n_conj, monomials)\n",
    "        results_by_degree[dv].append(fourier_level_lp_norm(f, 2))\n",
    "\n",
    "print(f\"Experiment 1: L_{{1,2}} vs degree d  (n = {n_conj}, {n_per_degree} samples/degree)\")\n",
    "print()\n",
    "print(f\"{'d':<6}{'mean L_{1,2}':<16}{'max L_{1,2}':<16}{'known bound':<16}{'d^2':<8}\")\n",
    "print(\"-\" * 62)\n",
    "for dv in degrees:\n",
    "    vals = results_by_degree[dv]\n",
    "    print(f\"{dv:<6}{np.mean(vals):<16.4f}{np.max(vals):<16.4f}{4 * 2**(6*dv):<16}{dv**2:<8}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# === Experiment 2: L_{1,2} vs n (fixed d = 2) ===\n",
    "# This is the key test: the conjecture says L_{1,2} depends only on d, not n.\n",
    "# C(n,2) grows quadratically, but L_{1,2} should stay flat.\n",
    "d_fixed = 2\n",
    "n_values = [4, 6, 8, 10, 12, 14, 16, 18]\n",
    "n_per_n = 120\n",
    "n_rng = np.random.default_rng(seed=456)\n",
    "\n",
    "results_by_n = {nv: [] for nv in n_values}\n",
    "\n",
    "for nv in n_values:\n",
    "    for _ in range(n_per_n):\n",
    "        monomials = []\n",
    "        for deg in range(1, d_fixed + 1):\n",
    "            for combo in combinations(range(nv), deg):\n",
    "                if n_rng.random() < 0.3:\n",
    "                    monomials.append(set(combo))\n",
    "        if not monomials:\n",
    "            monomials = [{0, 1}]\n",
    "        f = bf.f2_polynomial(nv, monomials)\n",
    "        results_by_n[nv].append(fourier_level_lp_norm(f, 2))\n",
    "\n",
    "print(f\"Experiment 2: L_{{1,2}} vs n  (d = {d_fixed}, {n_per_n} samples/n)\")\n",
    "print()\n",
    "print(f\"{'n':<8}{'C(n,2)':<10}{'mean L_{1,2}':<16}{'max L_{1,2}':<16}\")\n",
    "print(\"-\" * 50)\n",
    "for nv in n_values:\n",
    "    vals = results_by_n[nv]\n",
    "    print(f\"{nv:<8}{nv*(nv-1)//2:<10}{np.mean(vals):<16.4f}{np.max(vals):<16.4f}\")\n",
    "\n",
    "print(f\"\\nC(n,2) grows quadratically with n, but L_{{1,2}} stays bounded (and actually decreases).\")\n",
    "print(f\"The decrease is expected for *random* polynomials: as n grows, more monomials\")\n",
    "print(f\"cause the function to concentrate, shrinking individual Fourier coefficients.\")\n",
    "print(f\"The conjecture is about the *worst case* over all degree-d polynomials, not random ones.\")\n",
    "print(f\"Finding adversarial polynomials with large L_{{1,2}} at large n would be more informative.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "\n",
    "# Left: box plot of L_{1,2} by degree\n",
    "box_data = [results_by_degree[dv] for dv in degrees]\n",
    "bp = axes[0].boxplot(box_data, labels=[str(dv) for dv in degrees], patch_artist=True)\n",
    "for patch in bp[\"boxes\"]:\n",
    "    patch.set_facecolor(\"steelblue\")\n",
    "    patch.set_alpha(0.6)\n",
    "axes[0].set_xlabel(\"GF(2) degree $d$\")\n",
    "axes[0].set_ylabel(\"$L_{1,2}(f)$\")\n",
    "axes[0].set_title(f\"$L_{{1,2}}$ vs degree (n = {n_conj})\")\n",
    "axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Center: max L_{1,2} vs d, compared to d^2\n",
    "max_vals = [np.max(results_by_degree[dv]) for dv in degrees]\n",
    "mean_vals = [np.mean(results_by_degree[dv]) for dv in degrees]\n",
    "axes[1].plot(degrees, max_vals, \"s-\", color=\"steelblue\", label=\"max $L_{1,2}$ (empirical)\", linewidth=2)\n",
    "axes[1].plot(degrees, mean_vals, \"o-\", color=\"orange\", label=\"mean $L_{1,2}$ (empirical)\", linewidth=2)\n",
    "axes[1].plot(degrees, [dv**2 for dv in degrees], \"k--\", label=\"$d^2$ (Conjecture 3)\", linewidth=1.5)\n",
    "axes[1].set_xlabel(\"GF(2) degree $d$\")\n",
    "axes[1].set_ylabel(\"$L_{1,2}$\")\n",
    "axes[1].set_title(\"Empirical $L_{1,2}$ vs $d^2$\")\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Right: L_{1,2} vs n for fixed d=2\n",
    "max_by_n = [np.max(results_by_n[nv]) for nv in n_values]\n",
    "mean_by_n = [np.mean(results_by_n[nv]) for nv in n_values]\n",
    "axes[2].plot(n_values, max_by_n, \"s-\", color=\"steelblue\", label=\"max $L_{1,2}$\", linewidth=2)\n",
    "axes[2].plot(n_values, mean_by_n, \"o-\", color=\"orange\", label=\"mean $L_{1,2}$\", linewidth=2)\n",
    "axes[2].plot(n_values, [nv * (nv - 1) / 200 for nv in n_values], \"k:\", alpha=0.4, label=\"$\\\\binom{n}{2}/100$ (for scale)\")\n",
    "axes[2].set_xlabel(\"$n$ (number of variables)\")\n",
    "axes[2].set_ylabel(\"$L_{1,2}$\")\n",
    "axes[2].set_title(f\"$L_{{1,2}}$ vs $n$ (d = {d_fixed})\")\n",
    "axes[2].legend(fontsize=8)\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"The known bound 4 * 2^{{6d}} is not plotted -- it is off the chart.\")\n",
    "print(f\"For d=2 alone, the known bound is {4 * 2**12}; empirical max is {np.max(results_by_degree[2]):.4f}.\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We walked through the fractional PRG construction from CHLT'19:\n",
    "\n",
    "| Step | What | Paper reference |\n",
    "|------|------|-----------------|\n",
    "| Fourier tails | Compute $L_{1,2}$ for the test class | Theorem 2 |\n",
    "| Gaussian fooling | Small-variance, small-covariance Gaussians fool $\\tilde{f}$ | Theorem 9 (Appendix A) |\n",
    "| Balanced code | Matrix $A$ maps $\\ell$ Gaussians to $n$ with controlled covariance | Section 2, Step I |\n",
    "| Discretization | Approximate each Gaussian with $k$ bits | Lemma 10-11 |\n",
    "| Truncation | Clip to $[-1,1]^n$ | Claim 17 |\n",
    "\n",
    "The full Boolean PRG adds a **polarizing random walk** (Theorem 7, from [CHHL'18]) to round $[-1,1]^n$ to $\\{-1,+1\\}^n$.\n",
    "\n",
    "At $n = 16$, the construction uses more random bits than it saves. The seed length $\\ell = O((t/\\varepsilon)^{2+o(1)} \\cdot \\text{polylog}(n))$ only beats $n$ for large $n$ with moderate $t$. What matters at this scale is seeing each piece of the construction work.\n",
    "\n",
    "On Conjecture 3: the empirical $L_{1,2}$ values for random degree-$d$ polynomials were well within $O(d^2)$ and actually *decreased* with $n$. This is expected for random polynomials (concentration of measure), so it doesn't test the conjecture's worst case. The gap between empirical values and the known bound of $4 \\cdot 2^{6d}$ is large. Finding adversarial polynomials that *maximize* $L_{1,2}$ would be more informative than random sampling -- but even then, we'd only be testing specific instances, not the class.\n",
    "\n",
    "### Reference\n",
    "\n",
    "Chattopadhyay, Hatami, Lovett, Tal. [Pseudorandom Generators from the Second Fourier Level and Applications to AC0 with Parity Gates](https://doi.org/10.4230/LIPIcs.ITCS.2019.22). *ITCS 2019*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
