{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Fractional Pseudorandom Generator\n",
    "\n",
    "**Paper**: Chattopadhyay, Hatami, Lovett, Tal. [Pseudorandom Generators from the Second Fourier Level and Applications to AC0 with Parity Gates](https://doi.org/10.4230/LIPIcs.ITCS.2019.22). *ITCS 2019*.\n",
    "\n",
    "---\n",
    "\n",
    "This notebook walks through the construction of a **fractional pseudorandom generator** from the paper. The construction itself has several moving parts, so it helps to see each one happen concretely. At the end, we use the construction as an excuse to poke at Conjecture 3 on small instances.\n",
    "\n",
    "### What the paper does\n",
    "\n",
    "A PRG for a function class $\\mathcal{F}$ produces samples that no $f \\in \\mathcal{F}$ can distinguish from truly random inputs. The paper builds this in two stages:\n",
    "\n",
    "1. A **fractional PRG** -- samples in $[-1,1]^n$ that fool $f$'s multilinear extension, using only $\\ell \\ll n$ random values.\n",
    "2. A **polarizing random walk** (Theorem 7) converts the fractional output to Boolean $\\{-1,+1\\}^n$.\n",
    "\n",
    "We implement stage 1. The paper's target application is AC0[$\\oplus$] circuits, but the construction works for any restriction-closed class with bounded $L_{1,2}$. We test it on degree-$d$ polynomials over $\\mathbb{F}_2$, which is the class the paper focuses on.\n",
    "\n",
    "### Roadmap\n",
    "\n",
    "1. Background: function classes, Fourier tails, the $L_{1,2}$ condition\n",
    "2. Test functions: random degree-2 $\\mathbb{F}_2$ polynomials\n",
    "3. Theorem 9: small-variance Gaussians fool bounded-tail functions\n",
    "4. Dimension reduction via balanced codes\n",
    "5. The complete fractional PRG\n",
    "6. Poking at Conjecture 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "!pip install --upgrade boofun -q\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import boofun as bf\n",
    "from boofun.analysis.fourier import fourier_level_lp_norm\n",
    "from boofun.analysis.gf2 import gf2_degree\n",
    "from scipy.stats import norm\n",
    "from itertools import combinations\n",
    "\n",
    "np.random.seed(42)\n",
    "print(f\"boofun version: {bf.__version__}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Background\n",
    "\n",
    "### Function classes\n",
    "\n",
    "The paper works with a **class** $\\mathcal{F}$ of Boolean functions that is **closed under restrictions**: if $f \\in \\mathcal{F}$ and you fix some variables to constants, the restricted function is still in $\\mathcal{F}$. This property is needed because the PRG construction progressively restricts variables.\n",
    "\n",
    "The class we use: **degree-$d$ polynomials over $\\mathbb{F}_2$**, denoted $\\text{Poly}_{n,d}$. A member is $f(x) = (-1)^{p(x)}$ where $p : \\mathbb{F}_2^n \\to \\mathbb{F}_2$ is a polynomial of degree $\\leq d$. For example, with $d = 2$ on 4 variables:\n",
    "\n",
    "$$p(x) = x_0 x_1 \\oplus x_2 x_3 \\oplus x_0 \\qquad (\\text{degree 2})$$\n",
    "\n",
    "Fixing $x_0 = 1$ gives $p(x) = x_1 \\oplus x_2 x_3 \\oplus 1$, still degree $\\leq 2$. The class is restriction-closed.\n",
    "\n",
    "### Fourier tails\n",
    "\n",
    "Every $f: \\{-1,+1\\}^n \\to \\{-1,+1\\}$ has a unique multilinear expansion $\\tilde{f}(x) = \\sum_{S \\subseteq [n]} \\hat{f}(S) \\prod_{i \\in S} x_i$. The **level-$k$ Fourier tail** sums the absolute values of coefficients at degree $k$:\n",
    "\n",
    "$$L_{1,k}(f) = \\sum_{|S|=k} |\\hat{f}(S)|$$\n",
    "\n",
    "The paper's main result (Theorem 2): if $\\mathcal{F}$ is restriction-closed and $L_{1,2}(\\mathcal{F}) \\leq t$, then an explicit PRG exists with seed length $\\text{poly}(t, \\log n, 1/\\varepsilon)$. Only the second level matters.\n",
    "\n",
    "### What is $t$ for our class?\n",
    "\n",
    "For $\\text{Poly}_{n,d}$, the paper proves $L_{1,2}(\\text{Poly}_{n,d}) \\leq 4 \\cdot 2^{6d}$ (exponential in $d$). **Conjecture 3** says the truth is $O(d^2)$ -- polynomial, not exponential. If true, this would give PRGs for AC0[$\\oplus$].\n",
    "\n",
    "In the paper, $t$ is an analytical worst-case bound over the entire class. Here, we compute $L_{1,2}$ exactly for specific instances. The PRG we build would fool any function with $L_{1,2} \\leq t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Functions: Random Degree-2 $\\mathbb{F}_2$ Polynomials\n",
    "\n",
    "We generate random quadratic polynomials over $\\mathbb{F}_2$ on $n = 12$ variables. Each polynomial is a random subset of the $\\binom{12}{2} = 66$ quadratic monomials plus the 12 linear monomials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "n = 12\n",
    "d = 2\n",
    "\n",
    "\n",
    "def random_f2_poly(n, d, rng):\n",
    "    \"\"\"Generate a random degree-d polynomial over F2 on n variables.\"\"\"\n",
    "    monomials = []\n",
    "    # Include each possible monomial of degree 1..d with probability 1/2\n",
    "    for deg in range(1, d + 1):\n",
    "        for combo in combinations(range(n), deg):\n",
    "            if rng.random() < 0.5:\n",
    "                monomials.append(set(combo))\n",
    "    if not monomials:\n",
    "        # Avoid the constant function\n",
    "        monomials = [{0, 1}]\n",
    "    return bf.f2_polynomial(n, monomials)\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(seed=42)\n",
    "test_fns = {f\"quad_{i}\": random_f2_poly(n, d, rng) for i in range(5)}\n",
    "\n",
    "# Compute Fourier tails\n",
    "print(f\"Random degree-{d} F2 polynomials on n = {n} variables\")\n",
    "print(f\"Known bound: L_{{1,2}}(Poly_{{n,{d}}}) <= 4 * 2^(6*{d}) = {4 * 2**(6*d)}\")\n",
    "print(f\"Conjecture 3: L_{{1,2}} = O(d^2) = O({d**2})\")\n",
    "print()\n",
    "\n",
    "header = f\"{'Function':<12}{'GF2 deg':<10}\" + \"\".join(\n",
    "    f\"{'L_{1,' + str(k) + '}':<12}\" for k in range(5)\n",
    ")\n",
    "print(header)\n",
    "print(\"-\" * 82)\n",
    "\n",
    "l12_values = {}\n",
    "for name, func in test_fns.items():\n",
    "    gf2_d = gf2_degree(func)\n",
    "    row = f\"{name:<12}{gf2_d:<10}\"\n",
    "    for k in range(5):\n",
    "        val = fourier_level_lp_norm(func, k)\n",
    "        row += f\"{val:<12.4f}\"\n",
    "        if k == 2:\n",
    "            l12_values[name] = val\n",
    "    print(row)\n",
    "\n",
    "t = max(l12_values.values())\n",
    "print(f\"\\nEmpirical t = max L_{{1,2}} across our instances: {t:.4f}\")\n",
    "print(f\"Known bound for d={d}: {4 * 2**(6*d)}\")\n",
    "print(f\"Conjectured bound for d={d}: O({d**2})\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Fast batch evaluator: builds all 2^n monomials in O(n * 2^n) per batch\n",
    "def eval_multilinear_batch(fourier_coeffs, n_vars, X, batch_size=100):\n",
    "    \"\"\"Evaluate multilinear extension at rows of X using monomial-building trick.\"\"\"\n",
    "    m = X.shape[0]\n",
    "    size = len(fourier_coeffs)\n",
    "    results = np.zeros(m)\n",
    "    for start in range(0, m, batch_size):\n",
    "        end = min(start + batch_size, m)\n",
    "        batch = X[start:end]\n",
    "        b = batch.shape[0]\n",
    "        # vals[i, S] = product_{j in S} batch[i, j]\n",
    "        vals = np.zeros((b, size))\n",
    "        vals[:, 0] = 1.0\n",
    "        for i in range(n_vars):\n",
    "            step = 1 << i\n",
    "            vals[:, step : 2 * step] = vals[:, :step] * batch[:, i : i + 1]\n",
    "        results[start:end] = vals @ fourier_coeffs\n",
    "    return results"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Theorem 9: Gaussians Fool Bounded-Tail Functions\n",
    "\n",
    "The construction relies on this result (restating Raz-Tal):\n",
    "\n",
    "> **Theorem 9.** Let $Z \\in \\mathbb{R}^n$ be a zero-mean multivariate Gaussian with $\\text{Var}[Z_i] \\leq p$ and $|\\text{Cov}[Z_i, Z_j]| \\leq \\delta$ for $i \\neq j$. If $L_{1,2}(\\mathcal{F}) \\leq t$, then for any $f \\in \\mathcal{F}$:\n",
    ">\n",
    "> $$|\\mathbb{E}[\\tilde{f}(\\text{trnc}(Z))] - \\tilde{f}(\\mathbf{0})| \\leq O(\\delta \\cdot t)$$\n",
    "\n",
    "where $\\text{trnc}$ clips each coordinate to $[-1, 1]$, and $\\tilde{f}(\\mathbf{0}) = \\hat{f}(\\emptyset) = \\mathbb{E}[f]$.\n",
    "\n",
    "The error is $O(\\delta \\cdot t)$ because the level-1 terms vanish (zero mean), and the level-2 terms each pick up a factor of $\\delta$ (the covariance). Higher levels contribute negligibly when the variance is small.\n",
    "\n",
    "To achieve fooling error $\\varepsilon$: set $\\delta = \\varepsilon / t$.\n",
    "\n",
    "As a baseline, we first try $n$ **independent** Gaussians (trivially $\\delta = 0$, but no randomness savings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Parameters from Theorem 9\n",
    "eps = 0.3\n",
    "delta = eps / max(t, 1e-6)\n",
    "p_var = 1.0 / (8 * np.log(n / delta))  # variance per coordinate\n",
    "sigma = np.sqrt(p_var)\n",
    "\n",
    "print(\"Theorem 9 parameters:\")\n",
    "print(f\"  n = {n},  t = {t:.4f},  eps = {eps}\")\n",
    "print(f\"  delta = eps/t = {delta:.6f}\")\n",
    "print(f\"  p (variance) = {p_var:.6f},  sigma = {sigma:.6f}\")\n",
    "print()\n",
    "\n",
    "# --- Naive baseline: n independent Gaussians ---\n",
    "n_samples = 500\n",
    "naive_results = {}\n",
    "for name, func in test_fns.items():\n",
    "    fourier = func.fourier()\n",
    "    E_f = fourier[0]\n",
    "    Z = np.random.randn(n_samples, n) * sigma\n",
    "    X = np.clip(Z, -1, 1)\n",
    "    vals = eval_multilinear_batch(fourier, n, X)\n",
    "    err = abs(np.mean(vals) - E_f)\n",
    "    naive_results[name] = {\"E_f\": E_f, \"mean\": np.mean(vals), \"error\": err}\n",
    "    print(f\"  {name}: E[f]={E_f:+.4f}, naive mean={np.mean(vals):+.4f}, error={err:.4f}\")\n",
    "\n",
    "print(f\"\\nNaive approach: {n} independent random values per sample (no savings).\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dimension Reduction via Balanced Codes\n",
    "\n",
    "Instead of $n$ independent Gaussians, the paper generates $n$ *correlated* values from only $\\ell$ independent Gaussians.\n",
    "\n",
    "**Construction** (Paper, Section 2):\n",
    "1. Choose codewords $c_1, \\ldots, c_n \\in \\{0,1\\}^\\ell$ from a $\\delta$-balanced code\n",
    "2. Build $A \\in \\mathbb{R}^{n \\times \\ell}$: $\\; A_{i,j} = \\sqrt{p/\\ell} \\cdot (-1)^{c_{i,j}}$\n",
    "3. Sample $Y \\sim N(0, I_\\ell)$ -- only $\\ell$ random values\n",
    "4. Set $Z = AY$\n",
    "\n",
    "Each row of $A$ has squared norm $p$, so $\\text{Var}[Z_i] = p$ exactly. The covariance between $Z_i$ and $Z_j$ is $p \\cdot (1 - 2 d_H(c_i, c_j)/\\ell)$, where $d_H$ is the Hamming distance. A good code keeps all pairwise Hamming distances close to $\\ell/2$, which makes the covariances small.\n",
    "\n",
    "The required code length is $\\ell = O(\\log n / \\delta^{2+o(1)})$. At our scale ($n = 12$) the theoretical $\\ell$ exceeds $n$ -- the savings only appear for large $n$. We use $\\ell = 8$ here to show the mechanics of the construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Dimension reduction: n -> ell\n",
    "ell_theory = int(np.ceil((max(t, 1) / eps) ** 2 * np.log2(n) * 10))\n",
    "ell = 8  # for demonstration\n",
    "\n",
    "print(f\"Dimension reduction: {n} -> {ell}\")\n",
    "print(f\"  (Theoretical ell ~ {ell_theory}; at this scale ell > n.)\")\n",
    "print(f\"  (The savings appear when n is large and t is moderate.)\")\n",
    "print()\n",
    "\n",
    "# Balanced codewords: c_i in {0,1}^ell\n",
    "# The paper uses explicit small-biased spaces (Ta-Shma 2017).\n",
    "# Here: random codewords, then check properties.\n",
    "code_rng = np.random.default_rng(seed=7)\n",
    "codewords = code_rng.integers(0, 2, size=(n, ell))\n",
    "\n",
    "# Matrix A: A_{i,j} = sqrt(p / ell) * (-1)^{c_{i,j}}\n",
    "scale = np.sqrt(p_var / ell)\n",
    "A = scale * (1 - 2 * codewords.astype(float))\n",
    "\n",
    "# Covariance structure: Cov(Z) = A @ A^T\n",
    "cov_Z = A @ A.T\n",
    "diag = np.diag(cov_Z)\n",
    "off_diag = cov_Z[np.triu_indices(n, k=1)]\n",
    "effective_delta = np.max(np.abs(off_diag))\n",
    "\n",
    "print(\"Covariance structure of Z = AY:\")\n",
    "print(f\"  Var[Z_i] = {diag[0]:.6f}  (= p, by construction)\")\n",
    "print(f\"  max |Cov[Z_i, Z_j]| = {effective_delta:.6f}  (effective delta)\")\n",
    "print(f\"  mean |Cov[Z_i, Z_j]| = {np.mean(np.abs(off_diag)):.6f}\")\n",
    "print(f\"\\nTheoretical fooling bound: O(delta * t) = {effective_delta * t:.4f}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "im0 = axes[0].imshow(A, cmap=\"RdBu\", aspect=\"auto\")\n",
    "axes[0].set_xlabel(f\"Seed dimension ($\\ell$ = {ell})\")\n",
    "axes[0].set_ylabel(f\"Output dimension (n = {n})\")\n",
    "axes[0].set_title(f\"Matrix A  ({n} x {ell})\")\n",
    "plt.colorbar(im0, ax=axes[0])\n",
    "\n",
    "im1 = axes[1].imshow(cov_Z, cmap=\"RdBu\")\n",
    "axes[1].set_title(f\"Covariance  $AA^T$  ({n} x {n})\")\n",
    "plt.colorbar(im1, ax=axes[1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. The Complete Fractional PRG\n",
    "\n",
    "**Discretization** (Paper, Lemma 10-11): each of the $\\ell$ continuous Gaussians is approximated by a discrete random variable using $k$ bits.\n",
    "\n",
    "The full pipeline:\n",
    "\n",
    "$$\\underbrace{s \\text{ bits}}_{\\text{seed}} \\;\\xrightarrow{\\text{quantize}}\\; \\underbrace{Y \\in \\mathbb{R}^\\ell}_{\\text{approx. Gaussians}} \\;\\xrightarrow{Z = AY}\\; \\underbrace{Z \\in \\mathbb{R}^n}_{\\text{correlated}} \\;\\xrightarrow{\\text{trnc}}\\; \\underbrace{X \\in [-1,1]^n}_{\\text{output}}$$\n",
    "\n",
    "We check: $|\\mathbb{E}[\\tilde{f}(X)] - \\mathbb{E}[f]| \\leq O(\\delta \\cdot t)$ for each test function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# --- Discretization ---\n",
    "bits_per_coord = 8\n",
    "num_levels = 2**bits_per_coord\n",
    "total_seed_bits = ell * bits_per_coord\n",
    "\n",
    "# Quantized Gaussian: 256-level approximation of N(0,1)\n",
    "quantized_gaussian = norm.ppf((np.arange(num_levels) + 0.5) / num_levels)\n",
    "\n",
    "print(f\"Discretization: {bits_per_coord} bits/coord, {num_levels} levels\")\n",
    "print(f\"Total seed: {ell} x {bits_per_coord} = {total_seed_bits} bits\")\n",
    "print()\n",
    "\n",
    "# === Complete Fractional PRG ===\n",
    "n_trials = 500\n",
    "\n",
    "print(f\"Verification ({n_trials} trials)\")\n",
    "print(f\"  Seed: {total_seed_bits} bits -> {ell} Gaussians -> {n} fractional values\")\n",
    "print()\n",
    "\n",
    "prg_results = {}\n",
    "for name, func in test_fns.items():\n",
    "    fourier = func.fourier()\n",
    "    E_f = fourier[0]\n",
    "\n",
    "    seeds = np.random.randint(0, num_levels, size=(n_trials, ell))\n",
    "    Y_batch = quantized_gaussian[seeds]\n",
    "    Z_batch = Y_batch @ A.T\n",
    "    X_batch = np.clip(Z_batch, -1, 1)\n",
    "\n",
    "    vals = eval_multilinear_batch(fourier, n, X_batch)\n",
    "    prg_mean = np.mean(vals)\n",
    "    prg_err = abs(prg_mean - E_f)\n",
    "    theoretical = effective_delta * t\n",
    "\n",
    "    prg_results[name] = {\"E_f\": E_f, \"mean\": prg_mean, \"error\": prg_err}\n",
    "\n",
    "    print(f\"  {name}:\")\n",
    "    print(f\"    E[f] = {E_f:+.4f}\")\n",
    "    print(f\"    PRG mean  = {prg_mean:+.4f}  (error = {prg_err:.4f})\")\n",
    "    print(f\"    Naive mean = {naive_results[name]['mean']:+.4f}  (error = {naive_results[name]['error']:.4f})\")\n",
    "    print(f\"    Theoretical bound ~ {theoretical:.4f}\")\n",
    "    print()\n",
    "\n",
    "# --- Comparison plot ---\n",
    "names = list(test_fns.keys())\n",
    "x_pos = np.arange(len(names))\n",
    "width = 0.25\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "naive_errs = [naive_results[nm][\"error\"] for nm in names]\n",
    "prg_errs = [prg_results[nm][\"error\"] for nm in names]\n",
    "\n",
    "axes[0].bar(x_pos - width / 2, naive_errs, width, label=f\"Naive ({n} vals)\", color=\"steelblue\")\n",
    "axes[0].bar(x_pos + width / 2, prg_errs, width, label=f\"PRG ({ell} vals)\", color=\"orange\")\n",
    "axes[0].axhline(\n",
    "    y=effective_delta * t,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"O(delta * t) ~ {effective_delta * t:.4f}\",\n",
    ")\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(names, fontsize=8, rotation=15)\n",
    "axes[0].set_ylabel(\"|mean - E[f]|\")\n",
    "axes[0].set_title(\"Fooling Error\")\n",
    "axes[0].legend(fontsize=7)\n",
    "axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Seed length scaling\n",
    "ns = [16, 64, 256, 1024, 4096, 16384, 65536]\n",
    "ell_values = [int(np.ceil((max(t, 1) / eps) ** 2 * np.log2(nn) * 5)) for nn in ns]\n",
    "axes[1].plot(ns, ns, \"k--\", label=\"n (naive)\", linewidth=2)\n",
    "axes[1].plot(ns, ell_values, \"o-\", color=\"orange\", label=r\"$\\ell$ (PRG)\", linewidth=2)\n",
    "crossover = next((nn for nn, ev in zip(ns, ell_values) if ev < nn), None)\n",
    "if crossover:\n",
    "    axes[1].axvline(\n",
    "        x=crossover, color=\"green\", linestyle=\":\", alpha=0.7, label=f\"Crossover ~ n={crossover}\"\n",
    "    )\n",
    "axes[1].set_xlabel(\"n (number of variables)\")\n",
    "axes[1].set_ylabel(\"Random values needed\")\n",
    "axes[1].set_title(r\"$\\ell$ vs n\")\n",
    "axes[1].set_xscale(\"log\")\n",
    "axes[1].set_yscale(\"log\")\n",
    "axes[1].legend(fontsize=8)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Poking at Conjecture 3\n",
    "\n",
    "**Conjecture 3** from the paper:\n",
    "\n",
    "> $L_{1,2}(\\text{Poly}_{n,d}) = O(d^2)$. That is, if $p : \\mathbb{F}_2^n \\to \\mathbb{F}_2$ has degree $d$ and $f(x) = (-1)^{p(x)}$, then $\\sum_{i<j} |\\hat{f}(\\{i,j\\})| = O(d^2)$.\n",
    "\n",
    "This is unproven. The known bound is $4 \\cdot 2^{6d}$ (exponential). If the conjecture holds, it gives PRGs for AC0[$\\oplus$].\n",
    "\n",
    "We can't say anything about the conjecture from small experiments. But we can compute $L_{1,2}$ for many random instances and see what the numbers look like. Take this as an anecdote, not evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Generate many random F2 polynomials at various degrees and measure L_{1,2}\n",
    "n_conj = 12\n",
    "degrees = [1, 2, 3, 4, 5]\n",
    "n_samples_per_degree = 80\n",
    "conj_rng = np.random.default_rng(seed=123)\n",
    "\n",
    "results_by_degree = {d_val: [] for d_val in degrees}\n",
    "\n",
    "for d_val in degrees:\n",
    "    for _ in range(n_samples_per_degree):\n",
    "        monomials = []\n",
    "        for deg in range(1, d_val + 1):\n",
    "            for combo in combinations(range(n_conj), deg):\n",
    "                if conj_rng.random() < 0.3:  # include each monomial with prob 0.3\n",
    "                    monomials.append(set(combo))\n",
    "        if not monomials:\n",
    "            monomials = [{0}]\n",
    "        f = bf.f2_polynomial(n_conj, monomials)\n",
    "        l12 = fourier_level_lp_norm(f, 2)\n",
    "        results_by_degree[d_val].append(l12)\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"L_{{1,2}} for random degree-d F2 polynomials on n = {n_conj} variables\")\n",
    "print(f\"({n_samples_per_degree} samples per degree)\")\n",
    "print()\n",
    "print(f\"{'d':<6}{'mean L_{1,2}':<16}{'max L_{1,2}':<16}{'known bound':<16}{'d^2':<8}\")\n",
    "print(\"-\" * 62)\n",
    "for d_val in degrees:\n",
    "    vals = results_by_degree[d_val]\n",
    "    known = 4 * 2 ** (6 * d_val)\n",
    "    print(\n",
    "        f\"{d_val:<6}{np.mean(vals):<16.4f}{np.max(vals):<16.4f}{known:<16}{d_val**2:<8}\"\n",
    "    )"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: box plot of L_{1,2} by degree\n",
    "box_data = [results_by_degree[d_val] for d_val in degrees]\n",
    "bp = axes[0].boxplot(box_data, labels=[str(d_val) for d_val in degrees], patch_artist=True)\n",
    "for patch in bp[\"boxes\"]:\n",
    "    patch.set_facecolor(\"steelblue\")\n",
    "    patch.set_alpha(0.6)\n",
    "axes[0].set_xlabel(\"GF(2) degree d\")\n",
    "axes[0].set_ylabel(\"$L_{1,2}(f)$\")\n",
    "axes[0].set_title(f\"$L_{{1,2}}$ of random degree-$d$ polynomials (n={n_conj})\")\n",
    "axes[0].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "# Right: max L_{1,2} vs d, compared to d^2 and 4*2^{6d}\n",
    "max_vals = [np.max(results_by_degree[d_val]) for d_val in degrees]\n",
    "mean_vals = [np.mean(results_by_degree[d_val]) for d_val in degrees]\n",
    "\n",
    "axes[1].plot(degrees, max_vals, \"s-\", color=\"steelblue\", label=\"max $L_{1,2}$ (empirical)\", linewidth=2)\n",
    "axes[1].plot(degrees, mean_vals, \"o-\", color=\"orange\", label=\"mean $L_{1,2}$ (empirical)\", linewidth=2)\n",
    "axes[1].plot(\n",
    "    degrees,\n",
    "    [d_val**2 for d_val in degrees],\n",
    "    \"k--\",\n",
    "    label=\"$d^2$ (Conjecture 3)\",\n",
    "    linewidth=1.5,\n",
    ")\n",
    "axes[1].set_xlabel(\"GF(2) degree d\")\n",
    "axes[1].set_ylabel(\"$L_{1,2}$\")\n",
    "axes[1].set_title(\"Empirical $L_{1,2}$ vs conjectured bound\")\n",
    "axes[1].legend(fontsize=9)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The known bound 4 * 2^{6d} is not plotted -- it would be off the chart.\")\n",
    "print(f\"For d=2 alone, the known bound is {4 * 2**12}.\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We walked through the fractional PRG construction from CHLT'19:\n",
    "\n",
    "| Step | What | Paper reference |\n",
    "|------|------|-----------------|\n",
    "| Fourier tails | Compute $L_{1,2}$ for the test class | Theorem 2 |\n",
    "| Gaussian fooling | Small-variance, small-covariance Gaussians fool $\\tilde{f}$ | Theorem 9 |\n",
    "| Balanced code | Matrix $A$ maps $\\ell$ Gaussians to $n$ with controlled covariance | Section 2, Step I |\n",
    "| Discretization | Approximate each Gaussian with $k$ bits | Lemma 10-11 |\n",
    "| Truncation | Clip to $[-1,1]^n$ | Claim 17 |\n",
    "\n",
    "The full Boolean PRG adds a **polarizing random walk** (Theorem 7) to round $[-1,1]^n$ to $\\{-1,+1\\}^n$.\n",
    "\n",
    "At $n = 12$, the construction uses more random bits than it saves. The seed length $\\ell = O((t/\\varepsilon)^{2+o(1)} \\cdot \\text{polylog}(n))$ only beats $n$ for large $n$ with moderate $t$. What matters at this scale is seeing each piece of the construction work.\n",
    "\n",
    "On Conjecture 3: the empirical $L_{1,2}$ values we saw for random degree-$d$ polynomials were consistent with $O(d^2)$, and nowhere near the known bound of $4 \\cdot 2^{6d}$. This is on $n = 12$ with random instances, so it says nothing about the conjecture in general. But the gap between the empirical numbers and the known bound is hard to ignore.\n",
    "\n",
    "### Reference\n",
    "\n",
    "Chattopadhyay, Hatami, Lovett, Tal. [Pseudorandom Generators from the Second Fourier Level and Applications to AC0 with Parity Gates](https://doi.org/10.4230/LIPIcs.ITCS.2019.22). *ITCS 2019*."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
