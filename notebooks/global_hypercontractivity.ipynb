{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Global Hypercontractivity: An Interactive Exploration\n",
        "\n",
        "**Based on**: Keevash, Lifshitz, Long & Minzer - \"Global hypercontractivity and its applications\"\n",
        "\n",
        "This notebook provides an interactive exploration of one of the most important recent advances in Boolean function analysis: **hypercontractivity for global functions under p-biased measures**.\n",
        "\n",
        "---\n",
        "\n",
        "## The Big Picture\n",
        "\n",
        "Classical hypercontractivity (Bonami-Beckner) is the cornerstone of Boolean function analysis:\n",
        "$$\\|T_\\rho f\\|_4 \\leq \\|f\\|_2 \\quad \\text{for } \\rho \\leq 1/\\sqrt{3}$$\n",
        "\n",
        "This powers fundamental results like:\n",
        "- **KKL Theorem**: Every non-trivial function has an influential variable\n",
        "- **Friedgut's Junta Theorem**: Low-influence functions are approximately juntas\n",
        "- **Invariance Principle**: Boolean functions behave like Gaussian functions\n",
        "\n",
        "**The Problem**: These results only work for the *uniform measure* ($p = 1/2$). For p-biased measures with small $p$, standard hypercontractivity fails!\n",
        "\n",
        "**The Solution**: Keevash et al. prove that hypercontractivity *does* hold for **global functions** - functions that don't depend too much on any small set of coordinates.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install/upgrade boofun (required for Colab)\n",
        "# This ensures you have the latest version with all features\n",
        "!pip install --upgrade boofun -q\n",
        "\n",
        "import boofun as bf\n",
        "print(f\"BooFun version: {bf.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "import boofun as bf\n",
        "from boofun.analysis import SpectralAnalyzer\n",
        "from scipy.special import comb\n",
        "from typing import Callable, List, Dict, Tuple\n",
        "from itertools import combinations\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up nice plotting\n",
        "plt.rcParams['figure.figsize'] = (12, 5)\n",
        "plt.rcParams['font.size'] = 11\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\u2713 Libraries loaded (including SpectralAnalyzer)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part I: p-Biased Fourier Analysis\n",
        "\n",
        "### The p-Biased Measure\n",
        "\n",
        "Under the **p-biased measure** $\\mu_p$, each bit is independently 1 with probability $p$:\n",
        "$$\\mu_p(x) = p^{|x|}(1-p)^{n-|x|}$$\n",
        "\n",
        "The **p-biased characters** are:\n",
        "$$\\chi_i^p(x) = \\frac{x_i - p}{\\sigma} \\quad \\text{where } \\sigma = \\sqrt{p(1-p)}$$\n",
        "\n",
        "These form an orthonormal basis: $\\mathbf{E}_{\\mu_p}[\\chi_S^p \\chi_T^p] = \\mathbf{1}_{S=T}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def p_biased_character(x: np.ndarray, S: set, p: float) -> float:\n",
        "    \"\"\"Compute \u03c7_S^p(x) = \u03a0_{i\u2208S} (x_i - p)/\u03c3.\"\"\"\n",
        "    sigma = np.sqrt(p * (1 - p))\n",
        "    if sigma == 0:\n",
        "        return 0.0\n",
        "    result = 1.0\n",
        "    for i in S:\n",
        "        result *= (x[i] - p) / sigma\n",
        "    return result\n",
        "\n",
        "def p_biased_expectation(f: Callable, n: int, p: float, samples: int = 10000) -> float:\n",
        "    \"\"\"E_\u03bcp[f(x)] via Monte Carlo.\"\"\"\n",
        "    total = 0.0\n",
        "    for _ in range(samples):\n",
        "        x = (np.random.random(n) < p).astype(int)\n",
        "        total += f(x)\n",
        "    return total / samples\n",
        "\n",
        "# Example: Majority under different p\n",
        "n = 9\n",
        "maj = bf.majority(n)\n",
        "\n",
        "p_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "print(\"E_\u03bcp[Majority_9] for different p:\")\n",
        "print(\"-\" * 40)\n",
        "for p in p_values:\n",
        "    exp = p_biased_expectation(lambda x: maj.evaluate(x), n, p)\n",
        "    print(f\"p = {p:.1f}: E[MAJ] = {exp:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why Standard Hypercontractivity Fails for Small p\n",
        "\n",
        "The standard Bonami-Beckner inequality states:\n",
        "$$\\|f\\|_4 \\leq 3^{d/2} \\|f\\|_2$$\n",
        "\n",
        "for degree-$d$ functions under the *uniform* measure.\n",
        "\n",
        "**Problem**: Under $\\mu_p$ with small $p$, the character $\\chi_i^p$ has a huge 4th moment:\n",
        "$$\\mathbf{E}_{\\mu_p}[(\\chi_i^p)^4] = \\sigma^{-2}((1-p)^3 + p^3) \\approx \\sigma^{-2} \\quad \\text{for small } p$$\n",
        "\n",
        "This blows up as $p \\to 0$!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the failure of standard hypercontractivity\n",
        "def lambda_p(p: float) -> float:\n",
        "    \"\"\"\u03bb(p) = E[(\u03c7_i^p)^4] - the 4th moment of p-biased character.\"\"\"\n",
        "    sigma = np.sqrt(p * (1 - p))\n",
        "    if sigma == 0:\n",
        "        return np.inf\n",
        "    return sigma**(-2) * ((1 - p)**3 + p**3)\n",
        "\n",
        "def sigma_p(p: float) -> float:\n",
        "    \"\"\"\u03c3(p) = \u221a(p(1-p)).\"\"\"\n",
        "    return np.sqrt(p * (1 - p))\n",
        "\n",
        "p_range = np.linspace(0.01, 0.99, 100)\n",
        "lambdas = [lambda_p(p) for p in p_range]\n",
        "sigmas = [sigma_p(p) for p in p_range]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot \u03bb(p)\n",
        "axes[0].plot(p_range, lambdas, 'b-', linewidth=2)\n",
        "axes[0].axhline(y=3, color='r', linestyle='--', alpha=0.7, label='\u03bb = 3 (uniform)')\n",
        "axes[0].set_xlabel('Bias p')\n",
        "axes[0].set_ylabel('\u03bb(p) = E[(\u03c7\u1d62)\u2074]')\n",
        "axes[0].set_title('4th Moment of p-Biased Character')\n",
        "axes[0].set_ylim(0, 50)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].annotate('Blows up!\\n(breaks hypercontractivity)', \n",
        "                 xy=(0.1, lambda_p(0.1)), xytext=(0.25, 35),\n",
        "                 arrowprops=dict(arrowstyle='->', color='red'),\n",
        "                 fontsize=10, color='red')\n",
        "\n",
        "# Plot \u03c3(p)\n",
        "axes[1].plot(p_range, sigmas, 'g-', linewidth=2)\n",
        "axes[1].axhline(y=0.5, color='r', linestyle='--', alpha=0.7, label='\u03c3 = 0.5 (uniform)')\n",
        "axes[1].set_xlabel('Bias p')\n",
        "axes[1].set_ylabel('\u03c3(p) = \u221a(p(1-p))')\n",
        "axes[1].set_title('Standard Deviation of p-Biased Bit')\n",
        "axes[1].legend()\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udcca Key observation:\")\n",
        "print(f\"   \u03bb(0.5) = {lambda_p(0.5):.2f} (manageable)\")\n",
        "print(f\"   \u03bb(0.1) = {lambda_p(0.1):.2f} (getting large)\")\n",
        "print(f\"   \u03bb(0.01) = {lambda_p(0.01):.2f} (HUGE!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part II: Generalized Influences and Global Functions\n",
        "\n",
        "### Generalized Influences\n",
        "\n",
        "For a set $S \\subseteq [n]$, the **S-derivative** is:\n",
        "$$D_S(f) = \\sum_{T \\supseteq S} \\hat{f}(T) \\chi_{T \\setminus S}$$\n",
        "\n",
        "The **generalized S-influence** is:\n",
        "$$I_S(f) = \\sigma^{-2|S|} \\|D_S(f)\\|_2^2 = \\sigma^{-2|S|} \\sum_{T \\supseteq S} \\hat{f}(T)^2$$\n",
        "\n",
        "**Note**: $I_{\\{i\\}}(f) = \\text{Inf}_i[f]$ (the usual influence).\n",
        "\n",
        "### Definition of Global Functions\n",
        "\n",
        "A function $f$ has **\u03b1-small generalized influences** if:\n",
        "$$I_S(f) \\leq \\alpha \\cdot \\mathbf{E}[f^2] \\quad \\text{for all } S \\subseteq [n]$$\n",
        "\n",
        "**Intuition**: A global function doesn't depend too heavily on any small set of coordinates."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generalized_influence(f, S: set, p: float = 0.5) -> float:\n",
        "    \"\"\"\n",
        "    Compute the generalized S-influence I_S(f).\n",
        "    \n",
        "    I_S(f) = \u03c3^{-2|S|} \u03a3_{T\u2287S} f\u0302(T)\u00b2\n",
        "    \"\"\"\n",
        "    n = f.n_vars\n",
        "    sigma = np.sqrt(p * (1 - p))\n",
        "    \n",
        "    # Get Fourier coefficients (under uniform measure for simplicity)\n",
        "    # Direct API: f.fourier()\n",
        "    fourier = f.fourier()\n",
        "    \n",
        "    # Sum over all T \u2287 S\n",
        "    total = 0.0\n",
        "    for T_idx in range(2**n):\n",
        "        T = set(i for i in range(n) if (T_idx >> i) & 1)\n",
        "        if S.issubset(T):\n",
        "            total += fourier[T_idx]**2\n",
        "    \n",
        "    if len(S) == 0:\n",
        "        return total\n",
        "    return sigma**(-2 * len(S)) * total\n",
        "\n",
        "def is_alpha_global(f, alpha: float, max_set_size: int = 3, p: float = 0.5) -> Tuple[bool, Dict]:\n",
        "    \"\"\"\n",
        "    Check if f has \u03b1-small generalized influences.\n",
        "    Returns (is_global, details).\n",
        "    \"\"\"\n",
        "    n = f.n_vars\n",
        "    ef2 = 1.0  # E[f\u00b2] = 1 for Boolean functions in \u00b11\n",
        "    \n",
        "    max_influence = 0.0\n",
        "    worst_set = set()\n",
        "    \n",
        "    # Check all sets up to max_set_size\n",
        "    for k in range(1, min(max_set_size + 1, n + 1)):\n",
        "        for S in combinations(range(n), k):\n",
        "            S_set = set(S)\n",
        "            inf_S = generalized_influence(f, S_set, p)\n",
        "            if inf_S > max_influence:\n",
        "                max_influence = inf_S\n",
        "                worst_set = S_set\n",
        "    \n",
        "    is_global = max_influence <= alpha * ef2\n",
        "    return is_global, {\n",
        "        'max_generalized_influence': max_influence,\n",
        "        'worst_set': worst_set,\n",
        "        'threshold': alpha * ef2\n",
        "    }\n",
        "\n",
        "# Test various functions\n",
        "n = 7\n",
        "functions = {\n",
        "    \"Majority\u2087\": bf.majority(n),\n",
        "    \"Dictator\": bf.dictator(0, n),  # dictator(index, n_vars)\n",
        "    \"Parity\u2087\": bf.parity(n),\n",
        "    \"Tribes(2,3)\": bf.tribes(2, 3),\n",
        "}\n",
        "\n",
        "print(\"Checking \u03b1-globality (\u03b1 = 0.5):\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "for name, f in functions.items():\n",
        "    is_global, details = is_alpha_global(f, alpha=0.5, max_set_size=2)\n",
        "    status = \"\u2713 GLOBAL\" if is_global else \"\u2717 NOT GLOBAL\"\n",
        "    print(f\"\\n{name}: {status}\")\n",
        "    print(f\"  Max I_S = {details['max_generalized_influence']:.4f} (at S = {details['worst_set']})\")\n",
        "    print(f\"  Threshold = {details['threshold']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part III: The Global Hypercontractivity Theorem\n",
        "\n",
        "### Theorem I.1.3 (Keevash-Lifshitz-Long-Minzer)\n",
        "\n",
        "Let $f \\in L^2(\\{0,1\\}^n, \\mu_p)$ with $I_S(f) \\leq \\alpha \\cdot \\mathbf{E}[f^2]$ for all $S$.\n",
        "\n",
        "Then:\n",
        "$$\\|T_{1/5} f\\|_4 \\leq \\alpha^{1/4} \\|f\\|_2$$\n",
        "\n",
        "**Key insight**: The dependence on $p$ is *hidden* in the condition on generalized influences!\n",
        "\n",
        "### The Replacement Method\n",
        "\n",
        "The proof interpolates from $\\mu_p$ to $\\mu_{1/2}$ using:\n",
        "$$f_t = \\sum_S \\hat{f}(S) \\chi_S^t$$\n",
        "\n",
        "where $\\chi_S^t$ uses $\\mu_{1/2}$ for coordinates $\\leq t$ and $\\mu_p$ for coordinates $> t$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def noise_operator_sample(f, x: np.ndarray, rho: float, p: float = 0.5) -> float:\n",
        "    \"\"\"\n",
        "    Estimate (T_\u03c1f)(x) via Monte Carlo.\n",
        "    y_i = x_i w.p. \u03c1, else resample from \u03bc_p.\n",
        "    \"\"\"\n",
        "    n = len(x)\n",
        "    samples = 500\n",
        "    total = 0.0\n",
        "    for _ in range(samples):\n",
        "        y = x.copy()\n",
        "        for i in range(n):\n",
        "            if np.random.random() > rho:\n",
        "                y[i] = int(np.random.random() < p)\n",
        "        total += (2 * f.evaluate(y) - 1)  # Convert to \u00b11\n",
        "    return total / samples\n",
        "\n",
        "def compute_lp_norm_p_biased(f, q: float, p: float, rho: float = None, samples: int = 2000) -> float:\n",
        "    \"\"\"Compute ||T_\u03c1f||_q under \u03bc_p (if rho given) or ||f||_q.\"\"\"\n",
        "    n = f.n_vars\n",
        "    total = 0.0\n",
        "    for _ in range(samples):\n",
        "        x = (np.random.random(n) < p).astype(int)\n",
        "        if rho is not None:\n",
        "            val = noise_operator_sample(f, x, rho, p)\n",
        "        else:\n",
        "            val = 2 * f.evaluate(x) - 1  # Convert to \u00b11\n",
        "        total += abs(val)**q\n",
        "    return (total / samples)**(1/q)\n",
        "\n",
        "# Verify global hypercontractivity numerically\n",
        "print(\"Numerical Verification of Global Hypercontractivity\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nTheorem: ||T_{1/5}f||_4 \u2264 \u03b1^{1/4} ||f||_2 for \u03b1-global f\")\n",
        "print()\n",
        "\n",
        "# Test with Majority (a global function)\n",
        "for n in [5, 7]:\n",
        "    maj = bf.majority(n)\n",
        "    \n",
        "    # Check globality\n",
        "    is_global, details = is_alpha_global(maj, alpha=1.0, max_set_size=2)\n",
        "    alpha = details['max_generalized_influence']\n",
        "    \n",
        "    # Compute norms\n",
        "    norm_2 = 1.0  # ||f||_2 = 1 for Boolean in \u00b11\n",
        "    \n",
        "    # ||T_{1/5}f||_4 (this is slow due to Monte Carlo)\n",
        "    norm_4_T = compute_lp_norm_p_biased(maj, 4, 0.5, rho=0.2, samples=500)\n",
        "    \n",
        "    # Bound\n",
        "    bound = alpha**(1/4) * norm_2\n",
        "    \n",
        "    status = \"\u2713\" if norm_4_T <= bound + 0.2 else \"?\"  # Allow some Monte Carlo error\n",
        "    print(f\"Majority_{n}: ||T_{{0.2}}f||_4 \u2248 {norm_4_T:.3f} \u2264 \u03b1^{{1/4}} = {bound:.3f} {status}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part IV: Applications to Sharp Thresholds\n",
        "\n",
        "### The Sharp Threshold Phenomenon\n",
        "\n",
        "A monotone function $f$ exhibits a **sharp threshold** if $\\mu_p(f)$ jumps from near 0 to near 1 in a small interval of $p$.\n",
        "\n",
        "**Bourgain's Theorem**: Global monotone functions have sharp thresholds.\n",
        "\n",
        "**Keevash et al. strengthen this**: They prove a quantitative bound relating the critical probability to the expectation threshold, making progress on the Kahn-Kalai conjecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def threshold_curve(f, p_range: np.ndarray, samples: int = 2000) -> np.ndarray:\n",
        "    \"\"\"Compute \u03bc_p(f) for each p in range.\"\"\"\n",
        "    n = f.n_vars\n",
        "    probs = []\n",
        "    for p in p_range:\n",
        "        count = 0\n",
        "        for _ in range(samples):\n",
        "            x = (np.random.random(n) < p).astype(int)\n",
        "            count += f.evaluate(x)\n",
        "        probs.append(count / samples)\n",
        "    return np.array(probs)\n",
        "\n",
        "# Compare threshold sharpness for global vs non-global functions\n",
        "p_range = np.linspace(0.01, 0.99, 40)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Global functions (Majority, Tribes) - should have sharp thresholds\n",
        "ax = axes[0]\n",
        "ax.set_title('Global Functions: Sharp Thresholds', fontsize=12)\n",
        "\n",
        "for n in [5, 9, 15]:\n",
        "    maj = bf.majority(n)\n",
        "    curve = threshold_curve(maj, p_range, samples=800)\n",
        "    ax.plot(p_range, curve, label=f'Majority_{n}', linewidth=2)\n",
        "\n",
        "# Add Tribes\n",
        "tribes = bf.tribes(3, 3)  # 9 variables\n",
        "curve_tribes = threshold_curve(tribes, p_range, samples=800)\n",
        "ax.plot(p_range, curve_tribes, '--', label='Tribes(3,3)', linewidth=2)\n",
        "\n",
        "ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n",
        "ax.set_xlabel('Bias p')\n",
        "ax.set_ylabel('\u03bc_p(f) = Pr[f(x)=1]')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Non-global function (Dictator) - no sharp threshold\n",
        "ax = axes[1]\n",
        "ax.set_title('Non-Global Functions: Gradual Threshold', fontsize=12)\n",
        "\n",
        "for i in range(3):\n",
        "    dict_f = bf.dictator(i, 9)  # dictator(index, n_vars)\n",
        "    curve = threshold_curve(dict_f, p_range, samples=800)\n",
        "    ax.plot(p_range, curve, label=f'Dictator(x_{i})', linewidth=2)\n",
        "\n",
        "# Also show AND (very high threshold)\n",
        "and_f = bf.AND(5)\n",
        "curve_and = threshold_curve(and_f, p_range, samples=800)\n",
        "ax.plot(p_range, curve_and, '--', label='AND_5', linewidth=2)\n",
        "\n",
        "ax.axhline(y=0.5, color='gray', linestyle=':', alpha=0.5)\n",
        "ax.set_xlabel('Bias p')\n",
        "ax.set_ylabel('\u03bc_p(f) = Pr[f(x)=1]')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Observation:\")\n",
        "print(\"   Global functions (Majority) have SHARP transitions around p=0.5\")\n",
        "print(\"   Non-global functions (Dictator) just follow \u03bc_p(f) = p (linear!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part V: The KKL Theorem and p-Biased Influences\n",
        "\n",
        "### Classical KKL (Uniform Measure)\n",
        "\n",
        "$$\\max_i \\text{Inf}_i[f] \\geq c \\cdot \\text{Var}[f] \\cdot \\frac{\\log n}{I[f]}$$\n",
        "\n",
        "**Interpretation**: Every non-constant Boolean function has an influential variable.\n",
        "\n",
        "### p-Biased KKL (Keevash et al.)\n",
        "\n",
        "For global functions under $\\mu_p$, an analogous statement holds!\n",
        "\n",
        "The key is that the *generalized influences* control the behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def p_biased_influence(f, i: int, p: float, samples: int = 3000) -> float:\n",
        "    \"\"\"Compute Inf_i^p[f] under \u03bc_p.\"\"\"\n",
        "    n = f.n_vars\n",
        "    total = 0.0\n",
        "    for _ in range(samples):\n",
        "        x = (np.random.random(n) < p).astype(int)\n",
        "        x0 = x.copy(); x0[i] = 0\n",
        "        x1 = x.copy(); x1[i] = 1\n",
        "        total += (f.evaluate(x0) != f.evaluate(x1))\n",
        "    return total / samples\n",
        "\n",
        "def p_biased_total_influence(f, p: float, samples: int = 2000) -> float:\n",
        "    \"\"\"Compute I^p[f] = \u03a3_i Inf_i^p[f].\"\"\"\n",
        "    return sum(p_biased_influence(f, i, p, samples) for i in range(f.n_vars))\n",
        "\n",
        "# Visualize how influences change with p\n",
        "n = 9\n",
        "maj = bf.majority(n)\n",
        "\n",
        "p_values = np.linspace(0.1, 0.9, 9)\n",
        "\n",
        "# Compute influences for each p\n",
        "influences_by_p = []\n",
        "total_influences = []\n",
        "\n",
        "print(\"Computing p-biased influences (this may take a moment)...\")\n",
        "for p in p_values:\n",
        "    infs = [p_biased_influence(maj, i, p, samples=500) for i in range(n)]\n",
        "    influences_by_p.append(infs)\n",
        "    total_influences.append(sum(infs))\n",
        "    print(f\"  p = {p:.2f} done\")\n",
        "\n",
        "# Plot\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Individual influences\n",
        "ax = axes[0]\n",
        "influences_by_p = np.array(influences_by_p)\n",
        "for i in range(min(5, n)):\n",
        "    ax.plot(p_values, influences_by_p[:, i], 'o-', label=f'Inf_{i+1}', alpha=0.7)\n",
        "ax.set_xlabel('Bias p')\n",
        "ax.set_ylabel('Influence Inf_i^p[f]')\n",
        "ax.set_title(f'p-Biased Influences of Majority_{n}')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Total influence\n",
        "ax = axes[1]\n",
        "ax.plot(p_values, total_influences, 'b-o', linewidth=2, markersize=8)\n",
        "ax.axhline(y=np.sqrt(n) * 2/np.pi, color='r', linestyle='--', \n",
        "           label=f'\u2248 2\u221an/\u03c0 (uniform limit)', alpha=0.7)\n",
        "ax.set_xlabel('Bias p')\n",
        "ax.set_ylabel('Total Influence I^p[f]')\n",
        "ax.set_title(f'p-Biased Total Influence of Majority_{n}')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udcca Key insight from KKL:\")\n",
        "print(f\"   For balanced Majority_{n}, max influence \u2248 {max(influences_by_p[4]):.3f}\")\n",
        "print(f\"   KKL lower bound \u2248 {0.1 * np.log(n) / np.sqrt(n):.3f}\")\n",
        "print(\"   \u2192 Majority satisfies KKL with equality (up to constants)!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part VI: The Invariance Principle (p-Biased Version)\n",
        "\n",
        "### Classical Invariance Principle (MOO 2010)\n",
        "\n",
        "For low-influence multilinear polynomials, the distribution of $f$ under Boolean inputs is close to Gaussian inputs.\n",
        "\n",
        "### p-Biased Invariance Principle (Keevash et al.)\n",
        "\n",
        "This extends to general $p$-biased measures for **global functions**!\n",
        "\n",
        "This has applications in:\n",
        "- Hardness of approximation\n",
        "- Social choice theory  \n",
        "- Extremal combinatorics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def multilinear_extension_at_gaussian(f, samples: int = 3000) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Sample the multilinear extension of f at Gaussian points.\n",
        "    Returns array of f(G) values where G ~ N(0,1)^n.\n",
        "    \"\"\"\n",
        "    n = f.n_vars\n",
        "    analyzer = SpectralAnalyzer(f)\n",
        "    fourier = analyzer.fourier_expansion()\n",
        "    \n",
        "    values = []\n",
        "    for _ in range(samples):\n",
        "        g = np.random.randn(n)\n",
        "        # Multilinear extension: f\u0303(g) = \u03a3_S f\u0302(S) \u03a0_{i\u2208S} g_i\n",
        "        val = 0.0\n",
        "        for S_idx in range(2**n):\n",
        "            if abs(fourier[S_idx]) > 1e-10:\n",
        "                prod = 1.0\n",
        "                for i in range(n):\n",
        "                    if (S_idx >> i) & 1:\n",
        "                        prod *= g[i]\n",
        "                val += fourier[S_idx] * prod\n",
        "        values.append(val)\n",
        "    return np.array(values)\n",
        "\n",
        "def boolean_distribution(f, p: float = 0.5, samples: int = 3000) -> np.ndarray:\n",
        "    \"\"\"Sample f(x) for x ~ \u03bc_p, returning \u00b11 values.\"\"\"\n",
        "    n = f.n_vars\n",
        "    values = []\n",
        "    for _ in range(samples):\n",
        "        x = (np.random.random(n) < p).astype(int)\n",
        "        values.append(2 * f.evaluate(x) - 1)  # Convert to \u00b11\n",
        "    return np.array(values)\n",
        "\n",
        "# Compare Boolean vs Gaussian for Majority\n",
        "n = 11\n",
        "maj = bf.majority(n)\n",
        "\n",
        "print(\"Computing distributions...\")\n",
        "bool_samples = boolean_distribution(maj, p=0.5, samples=2000)\n",
        "gauss_samples = multilinear_extension_at_gaussian(maj, samples=2000)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Histogram comparison\n",
        "ax = axes[0]\n",
        "ax.hist(bool_samples, bins=3, density=True, alpha=0.7, label='Boolean (uniform)', color='blue')\n",
        "ax.hist(gauss_samples, bins=50, density=True, alpha=0.5, label='Gaussian', color='orange')\n",
        "ax.set_xlabel('f(x)')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title(f'Invariance Principle: Majority_{n}')\n",
        "ax.legend()\n",
        "ax.set_xlim(-2, 2)\n",
        "\n",
        "# CDF comparison\n",
        "ax = axes[1]\n",
        "bool_sorted = np.sort(bool_samples)\n",
        "gauss_sorted = np.sort(gauss_samples)\n",
        "ax.plot(bool_sorted, np.linspace(0, 1, len(bool_sorted)), 'b-', \n",
        "        linewidth=2, label='Boolean CDF')\n",
        "ax.plot(gauss_sorted, np.linspace(0, 1, len(gauss_sorted)), 'r--', \n",
        "        linewidth=2, label='Gaussian CDF')\n",
        "ax.set_xlabel('f(x)')\n",
        "ax.set_ylabel('CDF')\n",
        "ax.set_title('CDF Comparison')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\ud83d\udcca Invariance Principle:\")\n",
        "print(f\"   Boolean mean: {np.mean(bool_samples):.4f}\")\n",
        "print(f\"   Gaussian mean: {np.mean(gauss_samples):.4f}\")\n",
        "print(f\"\\n   Boolean std: {np.std(bool_samples):.4f}\")\n",
        "print(f\"   Gaussian std: {np.std(gauss_samples):.4f}\")\n",
        "print(\"\\n   \u2192 For low-influence functions, these converge as n \u2192 \u221e!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part VII: Interactive Explorer\n",
        "\n",
        "Explore how different functions behave under p-biased measures!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def comprehensive_analysis(f, name: str, p_values: List[float] = [0.2, 0.5, 0.8]):\n",
        "    \"\"\"\n",
        "    Comprehensive p-biased analysis of a Boolean function.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"  Analysis of {name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    n = f.n_vars\n",
        "    print(f\"\\n\ud83d\udcca Basic Properties:\")\n",
        "    print(f\"   Variables: {n}\")\n",
        "    print(f\"   Degree: {f.degree()}\")\n",
        "    print(f\"   Balanced: {f.is_balanced()}\")\n",
        "    print(f\"   Monotone: {f.is_monotone()}\")\n",
        "    \n",
        "    # Check globality\n",
        "    is_global, details = is_alpha_global(f, alpha=0.5, max_set_size=2)\n",
        "    print(f\"\\n\ud83c\udf10 Globality (\u03b1=0.5):\")\n",
        "    print(f\"   Is global: {is_global}\")\n",
        "    print(f\"   Max I_S: {details['max_generalized_influence']:.4f}\")\n",
        "    print(f\"   Worst set: {details['worst_set']}\")\n",
        "    \n",
        "    # p-biased analysis\n",
        "    print(f\"\\n\ud83d\udcc8 p-Biased Measures:\")\n",
        "    print(f\"   {'p':<8} | {'\u03bc_p(f)':<10} | {'I^p[f]':<10} | {'max Inf_i':<10}\")\n",
        "    print(f\"   {'-'*45}\")\n",
        "    \n",
        "    for p in p_values:\n",
        "        mu_p = p_biased_expectation(lambda x: f.evaluate(x), n, p, samples=500)\n",
        "        infs = [p_biased_influence(f, i, p, samples=300) for i in range(min(n, 5))]\n",
        "        max_inf = max(infs)\n",
        "        total_inf = sum(infs) * n / min(n, 5)  # Estimate total\n",
        "        print(f\"   {p:<8.2f} | {mu_p:<10.4f} | {total_inf:<10.4f} | {max_inf:<10.4f}\")\n",
        "    \n",
        "    # Hypercontractivity check\n",
        "    print(f\"\\n\u26a1 Hypercontractivity:\")\n",
        "    alpha = details['max_generalized_influence']\n",
        "    bound = alpha**(1/4)\n",
        "    print(f\"   \u03b1 = {alpha:.4f}\")\n",
        "    print(f\"   Hypercontractive bound \u03b1^(1/4) = {bound:.4f}\")\n",
        "    print(f\"   Status: {'\u2713 Applicable' if is_global else '\u26a0 May not apply (not global)'}\")\n",
        "\n",
        "# Analyze several functions\n",
        "comprehensive_analysis(bf.majority(9), \"Majority\u2089\")\n",
        "comprehensive_analysis(bf.dictator(0, 9), \"Dictator(x\u2081)\")  # dictator(index, n_vars)\n",
        "comprehensive_analysis(bf.tribes(3, 3), \"Tribes(3,3)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary: Key Takeaways\n",
        "\n",
        "### 1. The Problem\n",
        "Standard hypercontractivity fails for p-biased measures with small p because the 4th moment of characters blows up.\n",
        "\n",
        "### 2. The Solution: Global Hypercontractivity\n",
        "**Theorem (KLLM)**: For functions with small *generalized* influences:\n",
        "$$\\|T_{1/5}f\\|_4 \\leq \\alpha^{1/4} \\|f\\|_2$$\n",
        "\n",
        "### 3. What Makes a Function \"Global\"?\n",
        "- No small set of coordinates has too much influence\n",
        "- Majority, Tribes are global; Dictator is NOT\n",
        "\n",
        "### 4. Applications\n",
        "- **Sharp Thresholds**: Global monotone functions have sharp thresholds\n",
        "- **KKL Theorem**: p-biased analog for global functions\n",
        "- **Invariance Principle**: p-biased generalization\n",
        "- **Extremal Combinatorics**: Tur\u00e1n numbers, hypergraph problems\n",
        "\n",
        "### 5. The Big Picture\n",
        "This result opens the door to applying Boolean function analysis techniques to **sparse** settings where $\\mu_p(f) = o(1)$, which is crucial for many open problems in TCS and combinatorics.\n",
        "\n",
        "---\n",
        "\n",
        "**Reference**: Keevash, Lifshitz, Long & Minzer. \"Global hypercontractivity and its applications.\" [arXiv:1906.05039](https://arxiv.org/abs/1906.05039)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\ud83c\udf93 Notebook complete!\")\n",
        "print(\"\\nThis notebook demonstrated:\")\n",
        "print(\"  1. p-Biased Fourier analysis\")\n",
        "print(\"  2. Generalized influences and global functions\")\n",
        "print(\"  3. The Global Hypercontractivity Theorem\")\n",
        "print(\"  4. Sharp threshold phenomena\")\n",
        "print(\"  5. KKL theorem for p-biased measures\")\n",
        "print(\"  6. The Invariance Principle\")\n",
        "print(\"\\nUsing the boofun library for all computations!\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
