{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Error Models for Boolean Function Analysis\n",
    "\n",
    "**Reference: O'Donnell, Chapters 7-8 (Learning, Noise Stability)**\n",
    "\n",
    "Real-world analysis of Boolean functions involves uncertainty.  Fourier\n",
    "coefficients estimated from samples have finite-sample error, noise corrupts\n",
    "evaluations, and PAC learning introduces controlled approximation.\n",
    "\n",
    "BooFun provides four error models that make these trade-offs explicit:\n",
    "\n",
    "| Model | Purpose |\n",
    "|---|---|\n",
    "| `ExactErrorModel` | No uncertainty — exact computation (default) |\n",
    "| `PACErrorModel` | Probably Approximately Correct bounds ($\\varepsilon, \\delta$) |\n",
    "| `NoiseErrorModel` | Bit-flip noise at a given rate |\n",
    "| `LinearErrorModel` | Automatic error propagation via the `uncertainties` library |\n",
    "\n",
    "This notebook demonstrates each model and shows how noise affects\n",
    "Fourier-analytic quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "!pip install --upgrade boofun -q\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import boofun as bf\n",
    "from boofun.core.errormodels import (\n",
    "    ExactErrorModel,\n",
    "    PACErrorModel,\n",
    "    NoiseErrorModel,\n",
    "    create_error_model,\n",
    ")\n",
    "\n",
    "print(f\"boofun version: {bf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exact Model — the Baseline\n",
    "\n",
    "By default BooFun uses exact computation.  The `ExactErrorModel` passes\n",
    "results through unchanged and always reports full confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact = ExactErrorModel()\n",
    "\n",
    "f = bf.majority(5)\n",
    "fourier = f.fourier()\n",
    "\n",
    "# The exact model is a no-op wrapper\n",
    "result = exact.apply_error(0.42)\n",
    "print(f\"apply_error(0.42) = {result}\")\n",
    "print(f\"confidence = {exact.get_confidence(result)}\")\n",
    "print(f\"reliable   = {exact.is_reliable(result)}\")\n",
    "\n",
    "# Show majority(5) Fourier spectrum as ground truth\n",
    "print(f\"\\nMajority(5) top Fourier coefficients:\")\n",
    "for s in range(len(fourier)):\n",
    "    if abs(fourier[s]) > 0.01:\n",
    "        bits = format(s, f'05b')\n",
    "        print(f\"  f_hat({bits}) = {fourier[s]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PAC Error Model\n",
    "\n",
    "The **PAC model** wraps results with $(\\varepsilon, \\delta)$ guarantees:\n",
    "with probability $\\geq 1 - \\delta$, the reported value is within\n",
    "$\\varepsilon$ of the true answer.\n",
    "\n",
    "This is the natural model for learning algorithms (Goldreich-Levin,\n",
    "junta learning, etc.) where you estimate Fourier coefficients from\n",
    "random samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pac_tight = PACErrorModel(epsilon=0.01, delta=0.05)\n",
    "pac_loose = PACErrorModel(epsilon=0.1, delta=0.2)\n",
    "\n",
    "true_value = 0.625  # E[majority(5)] in {0,1}\n",
    "\n",
    "for name, model in [(\"tight (eps=0.01, delta=0.05)\", pac_tight),\n",
    "                     (\"loose (eps=0.1,  delta=0.2)\", pac_loose)]:\n",
    "    result = model.apply_error(true_value)\n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  value       = {result['value']}\")\n",
    "    print(f\"  bounds      = [{result['lower_bound']:.3f}, {result['upper_bound']:.3f}]\")\n",
    "    print(f\"  confidence  = {result['confidence']:.0%}\")\n",
    "    print(f\"  reliable?   = {model.is_reliable(true_value)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining PAC bounds\n",
    "\n",
    "When two PAC estimates are combined (e.g. adding influence estimates),\n",
    "the union bound controls the combined failure probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = PACErrorModel(epsilon=0.05, delta=0.05)\n",
    "m2 = PACErrorModel(epsilon=0.05, delta=0.05)\n",
    "\n",
    "combined = m1.combine_pac_bounds(m1, m2, \"addition\")\n",
    "print(f\"Individual:  eps={m1.epsilon}, delta={m1.delta}\")\n",
    "print(f\"Combined:    eps={combined.epsilon}, delta={combined.delta}\")\n",
    "print(f\"Confidence:  {combined.confidence:.0%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Noise Model — Bit-Flip Errors\n",
    "\n",
    "The **noise model** simulates evaluation noise at a specified bit-flip\n",
    "rate $\\eta$.\n",
    "\n",
    "This connects directly to **noise stability** (O'Donnell Ch. 5):\n",
    "\n",
    "$$\\text{Stab}_\\rho[f] = \\sum_S \\rho^{|S|} \\hat{f}(S)^2$$\n",
    "\n",
    "where $\\rho = 1 - 2\\eta$.  Functions with high noise stability are\n",
    "robust; functions like parity are fragile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare noise impact on majority vs parity\n",
    "f_maj = bf.majority(5)\n",
    "f_par = bf.parity(5)\n",
    "n = 5\n",
    "N = 2 ** n\n",
    "\n",
    "noise_rates = [0.0, 0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "results = {\"majority\": [], \"parity\": []}\n",
    "\n",
    "np.random.seed(42)\n",
    "n_trials = 500\n",
    "\n",
    "for eta in noise_rates:\n",
    "    noisy = NoiseErrorModel(noise_rate=eta, random_seed=42)\n",
    "    for name, func in [(\"majority\", f_maj), (\"parity\", f_par)]:\n",
    "        correct = 0\n",
    "        for _ in range(n_trials):\n",
    "            x = np.random.randint(0, N)\n",
    "            true_val = bool(func.evaluate(x))\n",
    "            noisy_val = noisy.apply_error(true_val)\n",
    "            if noisy_val == true_val:\n",
    "                correct += 1\n",
    "        results[name].append(correct / n_trials)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(noise_rates, results[\"majority\"], 'bo-', label=\"Majority(5) — robust\")\n",
    "plt.plot(noise_rates, results[\"parity\"], 'rs--', label=\"Parity(5) — fragile\")\n",
    "plt.xlabel(\"Noise rate (η)\")\n",
    "plt.ylabel(\"Fraction correct\")\n",
    "plt.title(\"Noise Robustness: Majority vs Parity\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Compare with analytical noise stability\n",
    "print(\"\\nAnalytical noise stability (rho = 1 - 2*eta):\")\n",
    "for eta in [0.01, 0.1, 0.3]:\n",
    "    rho = 1 - 2 * eta\n",
    "    stab_maj = f_maj.noise_stability(rho)\n",
    "    stab_par = f_par.noise_stability(rho)\n",
    "    print(f\"  eta={eta:.2f}: Stab_maj={stab_maj:.4f}, Stab_par={stab_par:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How Noise Affects Fourier Coefficients\n",
    "\n",
    "Under the noise operator $T_\\rho$, Fourier coefficients are damped:\n",
    "\n",
    "$$\\widehat{T_\\rho f}(S) = \\rho^{|S|} \\hat{f}(S)$$\n",
    "\n",
    "High-degree coefficients vanish exponentially.  Let's visualise this\n",
    "for majority(5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bf.majority(5)\n",
    "fourier = f.fourier()\n",
    "n = 5\n",
    "\n",
    "rho_values = [1.0, 0.9, 0.7, 0.5, 0.3]\n",
    "fig, axes = plt.subplots(1, len(rho_values), figsize=(16, 3), sharey=True)\n",
    "\n",
    "for ax, rho in zip(axes, rho_values):\n",
    "    # Apply noise operator: dampen each coefficient by rho^|S|\n",
    "    damped = np.array([\n",
    "        fourier[s] * rho ** bin(s).count('1')\n",
    "        for s in range(len(fourier))\n",
    "    ])\n",
    "    ax.bar(range(len(damped)), damped, color='steelblue', alpha=0.7)\n",
    "    ax.set_title(f\"ρ = {rho}\")\n",
    "    ax.set_xlabel(\"Subset S\")\n",
    "    if ax == axes[0]:\n",
    "        ax.set_ylabel(\"Damped coeff\")\n",
    "\n",
    "fig.suptitle(\"Noise Operator T_ρ on Majority(5)\", fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Choosing an Error Model\n",
    "\n",
    "| Scenario | Recommended Model |\n",
    "|---|---|\n",
    "| Exact analysis of small functions (n ≤ 15) | `ExactErrorModel` (default) |\n",
    "| Learning from random samples | `PACErrorModel` |\n",
    "| Noisy oracle / measurement errors | `NoiseErrorModel` |\n",
    "| Propagating uncertainty through computations | `LinearErrorModel` |\n",
    "\n",
    "Use the factory to create models by name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_type in [\"exact\", \"pac\", \"noise\"]:\n",
    "    if model_type == \"pac\":\n",
    "        model = create_error_model(model_type, epsilon=0.05, delta=0.05)\n",
    "    elif model_type == \"noise\":\n",
    "        model = create_error_model(model_type, noise_rate=0.05)\n",
    "    else:\n",
    "        model = create_error_model(model_type)\n",
    "\n",
    "    print(f\"{model}\")\n",
    "    print(f\"  confidence = {model.get_confidence(0.5):.2f}\")\n",
    "    print(f\"  reliable   = {model.is_reliable(0.5)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- **Exact** is the default — use it when you can enumerate the full truth table.\n",
    "- **PAC** provides principled confidence intervals for learning/estimation.\n",
    "- **Noise** simulates the physical reality of noisy evaluation oracles.\n",
    "- **Linear** propagates uncertainty through chains of computations.\n",
    "\n",
    "The noise model connects directly to noise stability theory (Chapter 5)\n",
    "and the invariance principle (Chapter 11), while the PAC model connects\n",
    "to the learning results in Chapters 7-8."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
