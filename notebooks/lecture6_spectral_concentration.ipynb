{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GabbyTab/boofun/blob/main/notebooks/lecture6_spectral_concentration.ipynb)\n\n# CS 294-92: Lecture 6 - Spectral Concentration and Low-Degree Learning\n\n**Instructor:** Avishay Tal  \n**Scribe & Notebook by:** Gabriel Taboada  \n**Reference:** O'Donnell, *Analysis of Boolean Functions*, Chapter 3\n\n---\n\n## Overview\n\nThis notebook explores the connection between Boolean function analysis and learning theory:\n\n1. **Spectral Concentration**: When is a function's Fourier weight concentrated on low-degree coefficients?\n2. **Decision Trees**: How decision tree depth relates to spectral concentration\n3. **PAC Learning**: The LMN Theorem for learning functions with spectral concentration\n4. **Fourier Coefficient Estimation**: Using samples to estimate Fourier coefficients\n\n---"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install/upgrade boofun (required for Colab)\n",
        "# This ensures you have the latest version with all features\n",
        "!pip install --upgrade boofun -q\n",
        "\n",
        "import boofun as bf\n",
        "print(f\"BooFun version: {bf.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "sys.path.insert(0, '../src')\n",
        "\n",
        "import boofun as bf\n",
        "from boofun.analysis import learning, complexity\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"boofun loaded - using clean API\")\n",
        "print(f\"  bf.majority(5).total_influence() = {bf.majority(5).total_influence()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Spectral Concentration\n",
        "\n",
        "**Definition:** A function $f$ is **$\\varepsilon$-concentrated** on degree $\\leq k$ if:\n",
        "$$\\mathbf{W}^{>k}[f] := \\sum_{|S| > k} \\hat{f}(S)^2 \\leq \\varepsilon$$\n",
        "\n",
        "This means the \"Fourier weight\" on high-degree coefficients is small.\n",
        "\n",
        "### Example: Decision Trees and Spectral Concentration\n",
        "\n",
        "**Theorem (O'Donnell 3.2):** A depth-$d$ decision tree has all its Fourier mass on degree $\\leq d$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate spectral concentration for different functions\n",
        "# Using clean API: bf.dictator(), bf.majority(), etc.\n",
        "functions = [\n",
        "    (\"Dictator x\u2080\", bf.dictator(0, 4)),    # Depth 1\n",
        "    (\"Majority-5\", bf.majority(5)),         # Complex\n",
        "    (\"Parity-4\", bf.parity(4)),             # Max degree\n",
        "    (\"Tribes(2,3)\", bf.tribes(2, 3)),       # DNF\n",
        "]\n",
        "\n",
        "print(\"Spectral Concentration Analysis\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Function':<15} {'DT depth':>10} {'W\u22641':>10} {'W\u22642':>10} {'W\u22643':>10}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for name, f in functions:\n",
        "    dt_depth = complexity.decision_tree_depth(f)\n",
        "    \n",
        "    # Clean API: f.W_leq(k) for spectral concentration up to degree k\n",
        "    conc = [f.W_leq(k) for k in [1, 2, 3]]\n",
        "    \n",
        "    print(f\"{name:<15} {dt_depth:>10} {conc[0]:>10.4f} {conc[1]:>10.4f} {conc[2]:>10.4f}\")\n",
        "\n",
        "print(\"\\n\ud83d\udca1 Higher DT depth \u2192 spectral weight spreads to higher degrees\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Fourier Coefficient Estimation\n",
        "\n",
        "**Lemma (O'Donnell 3.30):** Given $m = O(\\log(1/\\delta)/\\varepsilon^2)$ samples, we can estimate $\\hat{f}(S)$ with error $\\leq \\varepsilon$ with probability $\\geq 1 - \\delta$.\n",
        "\n",
        "The empirical estimator is:\n",
        "$$\\tilde{f}(S) = \\frac{1}{m} \\sum_{i=1}^{m} f(x^{(i)}) \\chi_S(x^{(i)})$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate Fourier coefficient estimation\n",
        "from boofun.analysis.learning import FourierLearner\n",
        "\n",
        "# Create a test function using clean API\n",
        "f = bf.majority(4)\n",
        "true_coeffs = f.fourier()  # Clean: f.fourier() instead of SpectralAnalyzer(f).fourier_expansion()\n",
        "\n",
        "# Estimate coefficients with different sample sizes\n",
        "sample_sizes = [50, 200, 1000]\n",
        "\n",
        "print(\"Fourier Coefficient Estimation (Majority-4)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"{'Subset':<10} {'True':>12} \" + \"\".join(f\"{'m='+str(m):>15}\" for m in sample_sizes))\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Show estimation for a few important subsets\n",
        "subsets_to_show = [0, 1, 2, 4, 8, 3, 5, 15]  # Various subset masks\n",
        "\n",
        "for s in subsets_to_show:\n",
        "    subset = [i for i in range(4) if (s >> (3-i)) & 1]\n",
        "    true_val = true_coeffs[s]\n",
        "    \n",
        "    row = f\"{str(subset):<10} {true_val:>12.4f}\"\n",
        "    \n",
        "    for m in sample_sizes:\n",
        "        # Generate samples\n",
        "        rng = np.random.default_rng(42)\n",
        "        samples = []\n",
        "        labels = []\n",
        "        \n",
        "        for _ in range(m):\n",
        "            x = rng.integers(0, 16)\n",
        "            y = 1 - 2 * int(f.evaluate(x))  # Convert to \u00b11\n",
        "            samples.append(x)\n",
        "            labels.append(y)\n",
        "        \n",
        "        # Estimate using empirical formula\n",
        "        est = 0.0\n",
        "        for x, y in zip(samples, labels):\n",
        "            chi_s = 1 - 2 * (bin(x & s).count(\"1\") % 2)\n",
        "            est += y * chi_s\n",
        "        est /= m\n",
        "        \n",
        "        error = abs(est - true_val)\n",
        "        row += f\" {est:>8.4f}\u00b1{error:.4f}\"\n",
        "    \n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. The LMN Theorem (PAC Learning)\n",
        "\n",
        "**Theorem (Linial-Mansour-Nisan, 1993):** \n",
        "Let $\\mathcal{C}$ be a concept class such that every $f \\in \\mathcal{C}$ is $\\varepsilon$-concentrated on Fourier coefficients of degree $\\leq k$.\n",
        "Then $\\mathcal{C}$ is $(\\varepsilon, \\delta)$-PAC learnable in time $\\text{poly}(n^k, 1/\\varepsilon, \\log(1/\\delta))$.\n",
        "\n",
        "### Learning Algorithm\n",
        "\n",
        "1. Estimate all degree-$\\leq k$ Fourier coefficients using samples\n",
        "2. Construct hypothesis: $h(x) = \\text{sgn}\\left(\\sum_{|S| \\leq k} \\tilde{f}(S) \\chi_S(x)\\right)$\n",
        "3. Fourier concentration bounds the approximation error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate the LMN learning algorithm\n",
        "from boofun.analysis.learning import LowDegreeLearner\n",
        "\n",
        "# Create a depth-2 decision tree (has spectral concentration at degree \u2264 2)\n",
        "# (x\u2081 AND x\u2082) OR (x\u2083 AND x\u2084)\n",
        "tt = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1]\n",
        "target = bf.create(tt)\n",
        "\n",
        "print(\"LMN Learning Example\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Target: (x\u2081 AND x\u2082) OR (x\u2083 AND x\u2084)\")\n",
        "print(f\"Decision tree depth: {complexity.decision_tree_depth(target)}\")\n",
        "print()\n",
        "\n",
        "# Show spectral concentration using clean API: f.W_leq(k)\n",
        "for k in [1, 2, 3, 4]:\n",
        "    conc = target.W_leq(k)  # Clean: target.W_leq(k) instead of analyzer.spectral_concentration(k)\n",
        "    print(f\"W\u2264{k} = {conc:.4f}\")\n",
        "\n",
        "# Use the learner\n",
        "learner = LowDegreeLearner(n_vars=4, max_degree=2)\n",
        "samples = 200\n",
        "\n",
        "# Generate training data\n",
        "rng = np.random.default_rng(42)\n",
        "X = rng.integers(0, 16, size=samples)\n",
        "y = np.array([1 - 2 * int(target.evaluate(x)) for x in X])\n",
        "\n",
        "# Train\n",
        "learner.fit(X, y)\n",
        "\n",
        "# Test accuracy\n",
        "correct = 0\n",
        "for x in range(16):\n",
        "    true_label = 1 - 2 * int(target.evaluate(x))\n",
        "    pred_label = learner.predict(x)\n",
        "    if pred_label == true_label:\n",
        "        correct += 1\n",
        "\n",
        "print(f\"\\nLearning accuracy: {correct}/16 = {correct/16:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. The Goldreich-Levin Algorithm\n",
        "\n",
        "**Problem:** Find all \"heavy\" Fourier coefficients without enumerating all $2^n$ subsets.\n",
        "\n",
        "**Theorem (Goldreich-Levin, 1989):** There exists an algorithm that, given oracle access to $f$, finds all $S$ with $|\\hat{f}(S)| \\geq \\tau$ using $O(n/\\tau^4)$ queries.\n",
        "\n",
        "### Key Idea: Self-Correction via Restrictions\n",
        "\n",
        "For a random subset $T \\subseteq [n]$:\n",
        "$$\\mathbf{E}_{T,b}[\\hat{f|_{T=b}}(S|_{\\bar{T}})] = \\hat{f}(S)$$\n",
        "\n",
        "This allows recursively finding heavy coefficients by restricting variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate Goldreich-Levin algorithm\n",
        "from boofun.analysis.learning import GoldreichLevin\n",
        "\n",
        "# Create a sparse function using clean API\n",
        "xor = bf.parity(4)\n",
        "\n",
        "print(\"Goldreich-Levin Algorithm Demo\")\n",
        "print(\"=\" * 60)\n",
        "print(\"Target: XOR (Parity) on 4 bits\")\n",
        "print(f\"Sparsity: {xor.sparsity()} (only one non-zero Fourier coefficient)\")\n",
        "print()\n",
        "\n",
        "# True Fourier coefficients using clean API\n",
        "true_coeffs = xor.fourier()\n",
        "\n",
        "print(\"True heavy coefficients (|f\u0302(S)| > 0.1):\")\n",
        "for s in range(len(true_coeffs)):\n",
        "    if abs(true_coeffs[s]) > 0.1:\n",
        "        subset = [i for i in range(4) if (s >> (3-i)) & 1]\n",
        "        print(f\"  S={subset}: f\u0302(S) = {true_coeffs[s]:.4f}\")\n",
        "\n",
        "# Use Goldreich-Levin to find heavy coefficients\n",
        "gl = GoldreichLevin(xor.n_vars, threshold=0.3)\n",
        "\n",
        "# Define oracle function\n",
        "def oracle(x):\n",
        "    return 1 - 2 * int(xor.evaluate(x))\n",
        "\n",
        "heavy = gl.find_heavy_coefficients(oracle)\n",
        "\n",
        "print(f\"\\nGoldreich-Levin found {len(heavy)} heavy coefficient(s):\")\n",
        "for s, coeff in heavy:\n",
        "    subset = [i for i in range(4) if (s >> (3-i)) & 1]\n",
        "    print(f\"  S={subset}: estimated f\u0302(S) = {coeff:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## Summary\n",
        "\n",
        "### Key Concepts\n",
        "\n",
        "1. **Spectral Concentration**: Functions with bounded decision tree depth have Fourier weight concentrated on low degrees\n",
        "\n",
        "2. **LMN Theorem**: If a function class has spectral concentration at degree $k$, it's PAC-learnable in time $n^{O(k)}$\n",
        "\n",
        "3. **Fourier Coefficient Estimation**: $O(\\log(1/\\delta)/\\varepsilon^2)$ samples suffice to estimate any $\\hat{f}(S)$ within $\\varepsilon$\n",
        "\n",
        "4. **Goldreich-Levin**: Find heavy Fourier coefficients efficiently without enumeration\n",
        "\n",
        "### Corollaries (from lecture notes)\n",
        "\n",
        "- **Depth-d decision trees**: Learnable in time $n^{O(d)}$\n",
        "- **Size-s decision trees**: Learnable in time $n^{O(\\log s)}$\n",
        "- **Linear Threshold Functions**: Learnable in time $n^{O(1/\\varepsilon^2)}$\n",
        "\n",
        "### Open Questions\n",
        "\n",
        "- Can depth-$d$ decision trees be learned in $\\text{poly}(n, 2^d)$ time?\n",
        "- Can $k$-juntas be learned in $\\text{poly}(n)$ time for $k = \\log n$?\n",
        "- Efficient learning of small DNF/CNF formulas?"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
