{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GabbyTab/boofun/blob/main/notebooks/lecture6_spectral_concentration.ipynb)\n",
    "\n",
    "# CS 294-92: Lecture 6 - Spectral Concentration and Low-Degree Learning\n",
    "\n",
    "**Instructor:** Avishay Tal  \n",
    "**Scribe & Notebook by:** Gabriel Taboada  \n",
    "**Reference:** O'Donnell, *Analysis of Boolean Functions*, Chapter 3\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook explores the connection between Boolean function analysis and learning theory:\n",
    "\n",
    "1. **Spectral Concentration**: When is a function's Fourier weight concentrated on low-degree coefficients?\n",
    "2. **Decision Trees**: How decision tree depth relates to spectral concentration\n",
    "3. **PAC Learning**: The LMN Theorem for learning functions with spectral concentration\n",
    "4. **Fourier Coefficient Estimation**: Using samples to estimate Fourier coefficients\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:24:56.341459Z",
     "iopub.status.busy": "2026-02-02T02:24:56.341322Z",
     "iopub.status.idle": "2026-02-02T02:24:58.492697Z",
     "shell.execute_reply": "2026-02-02T02:24:58.492261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip\u001b[0m\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrieltaboada/dev/Boofun/boofun/src/boofun/core/errormodels.py:21: UserWarning: uncertainties library not available - some error models disabled\n",
      "  warnings.warn(\"uncertainties library not available - some error models disabled\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BooFun version: 1.1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gabrieltaboada/dev/Boofun/boofun/src/boofun/quantum/__init__.py:22: UserWarning: Qiskit not available - quantum features limited\n",
      "  warnings.warn(\"Qiskit not available - quantum features limited\")\n"
     ]
    }
   ],
   "source": [
    "# Install/upgrade boofun (required for Colab)\n",
    "# This ensures you have the latest version with all features\n",
    "!pip install --upgrade boofun -q\n",
    "\n",
    "import boofun as bf\n",
    "print(f\"BooFun version: {bf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:24:58.494034Z",
     "iopub.status.busy": "2026-02-02T02:24:58.493914Z",
     "iopub.status.idle": "2026-02-02T02:24:58.495592Z",
     "shell.execute_reply": "2026-02-02T02:24:58.495354Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import boofun as bf\n",
    "from boofun.analysis import learning, complexity\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spectral Concentration\n",
    "\n",
    "**Definition:** A function $f$ is **$\\varepsilon$-concentrated** on degree $\\leq k$ if:\n",
    "$$\\mathbf{W}^{>k}[f] := \\sum_{|S| > k} \\hat{f}(S)^2 \\leq \\varepsilon$$\n",
    "\n",
    "This means the \"Fourier weight\" on high-degree coefficients is small.\n",
    "\n",
    "### Example: Decision Trees and Spectral Concentration\n",
    "\n",
    "**Theorem (O'Donnell 3.2):** A depth-$d$ decision tree has all its Fourier mass on degree $\\leq d$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:24:58.496571Z",
     "iopub.status.busy": "2026-02-02T02:24:58.496519Z",
     "iopub.status.idle": "2026-02-02T02:24:58.499978Z",
     "shell.execute_reply": "2026-02-02T02:24:58.499779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectral Concentration Analysis\n",
      "======================================================================\n",
      "Function          DT depth        W≤1        W≤2        W≤3\n",
      "----------------------------------------------------------------------\n",
      "Dictator x₀              1     1.0000     1.0000     1.0000\n",
      "Majority-5               5     0.7031     0.7031     0.8594\n",
      "Parity-4                 4     0.0000     0.0000     0.0000\n",
      "Tribes(2,3)              3     0.7500     0.9375     1.0000\n",
      "\n",
      "Higher DT depth → spectral weight spreads to higher degrees\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate spectral concentration for different functions\n",
    "functions = [\n",
    "    (\"Dictator x₀\", bf.dictator(4, 0)),    # 4 vars, dictator index 0, depth 1\n",
    "    (\"Majority-5\", bf.majority(5)),         # Complex\n",
    "    (\"Parity-4\", bf.parity(4)),             # Max degree\n",
    "    (\"Tribes(2,3)\", bf.tribes(2, 3)),       # DNF\n",
    "]\n",
    "\n",
    "print(\"Spectral Concentration Analysis\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Function':<15} {'DT depth':>10} {'W≤1':>10} {'W≤2':>10} {'W≤3':>10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for name, f in functions:\n",
    "    dt_depth = complexity.decision_tree_depth(f)\n",
    "    \n",
    "    # f.W_leq(k) = sum of f̂(S)² for |S| ≤ k\n",
    "    conc = [f.W_leq(k) for k in [1, 2, 3]]\n",
    "    \n",
    "    print(f\"{name:<15} {dt_depth:>10} {conc[0]:>10.4f} {conc[1]:>10.4f} {conc[2]:>10.4f}\")\n",
    "\n",
    "print(\"\\nHigher DT depth → spectral weight spreads to higher degrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fourier Coefficient Estimation\n",
    "\n",
    "**Lemma (O'Donnell 3.30):** Given $m = O(\\log(1/\\delta)/\\varepsilon^2)$ samples, we can estimate $\\hat{f}(S)$ with error $\\leq \\varepsilon$ with probability $\\geq 1 - \\delta$.\n",
    "\n",
    "The empirical estimator is:\n",
    "$$\\tilde{f}(S) = \\frac{1}{m} \\sum_{i=1}^{m} f(x^{(i)}) \\chi_S(x^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:24:58.501045Z",
     "iopub.status.busy": "2026-02-02T02:24:58.500969Z",
     "iopub.status.idle": "2026-02-02T02:24:58.547348Z",
     "shell.execute_reply": "2026-02-02T02:24:58.547136Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourier Coefficient Estimation (Majority-4)\n",
      "======================================================================\n",
      "Subset S       True f̂(S)            m=50          m=200         m=1000\n",
      "----------------------------------------------------------------------\n",
      "[]                 0.3750   0.2400±0.1350   0.3800±0.0050   0.3680±0.0070\n",
      "[0]                0.3750   0.4800±0.1050   0.4600±0.0850   0.4160±0.0410\n",
      "[1]                0.3750   0.3600±0.0150   0.3100±0.0650   0.3860±0.0110\n",
      "[2]                0.3750   0.4000±0.0250   0.3400±0.0350   0.4020±0.0270\n",
      "[3]                0.3750   0.2000±0.1750   0.2500±0.1250   0.3220±0.0530\n",
      "[0, 1]            -0.1250  -0.1200±0.0050  -0.1900±0.0650  -0.1540±0.0290\n",
      "[0, 2]            -0.1250  -0.1600±0.0350  -0.1600±0.0350  -0.1220±0.0030\n",
      "[0, 1, 2, 3]       0.3750   0.3200±0.0550   0.4000±0.0250   0.3300±0.0450\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Fourier coefficient estimation\n",
    "f = bf.majority(4)\n",
    "true_coeffs = f.fourier()\n",
    "\n",
    "# Estimate coefficients with different sample sizes\n",
    "sample_sizes = [50, 200, 1000]\n",
    "\n",
    "print(\"Fourier Coefficient Estimation (Majority-4)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'Subset S':<12} {'True f̂(S)':>12} \" + \"\".join(f\"{'m='+str(m):>15}\" for m in sample_sizes))\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# Show estimation for a few important subsets (LSB=x₀ convention)\n",
    "subsets_to_show = [0, 1, 2, 4, 8, 3, 5, 15]  # Various subset masks\n",
    "\n",
    "for s in subsets_to_show:\n",
    "    # LSB=x₀: bit i of s means variable i is in subset\n",
    "    subset = [i for i in range(4) if (s >> i) & 1]\n",
    "    true_val = true_coeffs[s]\n",
    "    \n",
    "    row = f\"{str(subset):<12} {true_val:>12.4f}\"\n",
    "    \n",
    "    for m in sample_sizes:\n",
    "        # Generate samples\n",
    "        rng = np.random.default_rng(42)\n",
    "        samples = []\n",
    "        labels = []\n",
    "        \n",
    "        for _ in range(m):\n",
    "            x = int(rng.integers(0, 16))  # Convert to Python int\n",
    "            y = 1 - 2 * int(f.evaluate(x))  # Convert to ±1\n",
    "            samples.append(x)\n",
    "            labels.append(y)\n",
    "        \n",
    "        # Estimate using empirical formula: f̂(S) ≈ (1/m) Σ f(x) χ_S(x)\n",
    "        est = 0.0\n",
    "        for x, y in zip(samples, labels):\n",
    "            chi_s = 1 - 2 * (bin(x & s).count(\"1\") % 2)\n",
    "            est += y * chi_s\n",
    "        est /= m\n",
    "        \n",
    "        error = abs(est - true_val)\n",
    "        row += f\" {est:>8.4f}±{error:.4f}\"\n",
    "    \n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. The LMN Theorem (PAC Learning)\n",
    "\n",
    "**Theorem (Linial-Mansour-Nisan, 1993):** \n",
    "Let $\\mathcal{C}$ be a concept class such that every $f \\in \\mathcal{C}$ is $\\varepsilon$-concentrated on Fourier coefficients of degree $\\leq k$.\n",
    "Then $\\mathcal{C}$ is $(\\varepsilon, \\delta)$-PAC learnable in time $\\text{poly}(n^k, 1/\\varepsilon, \\log(1/\\delta))$.\n",
    "\n",
    "### Learning Algorithm\n",
    "\n",
    "1. Estimate all degree-$\\leq k$ Fourier coefficients using samples\n",
    "2. Construct hypothesis: $h(x) = \\text{sgn}\\left(\\sum_{|S| \\leq k} \\tilde{f}(S) \\chi_S(x)\\right)$\n",
    "3. Fourier concentration bounds the approximation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:24:58.548383Z",
     "iopub.status.busy": "2026-02-02T02:24:58.548324Z",
     "iopub.status.idle": "2026-02-02T02:24:58.577230Z",
     "shell.execute_reply": "2026-02-02T02:24:58.576815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMN Learning Example\n",
      "============================================================\n",
      "Target: (x₀ AND x₁) OR (x₂ AND x₃)\n",
      "Decision tree depth: 4\n",
      "\n",
      "W≤1 = 0.5781\n",
      "W≤2 = 0.9219\n",
      "W≤3 = 0.9844\n",
      "W≤4 = 1.0000\n",
      "\n",
      "LMN approach: estimate low-degree coefficients, threshold to classify\n",
      "\n",
      "Estimated 11 non-negligible degree-≤2 coefficients\n",
      "Learning accuracy: 16/16 = 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate the LMN learning algorithm (simplified version)\n",
    "from boofun.analysis.learning import estimate_fourier_coefficient\n",
    "\n",
    "# Create a depth-2 decision tree: (x₀ AND x₁) OR (x₂ AND x₃)\n",
    "# Truth table: f=1 when (x₀=1 and x₁=1) or (x₂=1 and x₃=1)\n",
    "tt = []\n",
    "for i in range(16):\n",
    "    x0, x1, x2, x3 = [(i >> j) & 1 for j in range(4)]\n",
    "    val = (x0 and x1) or (x2 and x3)\n",
    "    tt.append(val)\n",
    "target = bf.create(tt)\n",
    "\n",
    "print(\"LMN Learning Example\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Target: (x₀ AND x₁) OR (x₂ AND x₃)\")\n",
    "print(f\"Decision tree depth: {complexity.decision_tree_depth(target)}\")\n",
    "print()\n",
    "\n",
    "# Show spectral concentration: W≤k = sum of f̂(S)² for |S| ≤ k\n",
    "for k in [1, 2, 3, 4]:\n",
    "    conc = target.W_leq(k)\n",
    "    print(f\"W≤{k} = {conc:.4f}\")\n",
    "\n",
    "print(\"\\nLMN approach: estimate low-degree coefficients, threshold to classify\")\n",
    "\n",
    "# Estimate degree ≤ 2 coefficients\n",
    "rng = np.random.default_rng(42)\n",
    "estimated_coeffs = {}\n",
    "for s in range(16):\n",
    "    degree = bin(s).count('1')\n",
    "    if degree <= 2:\n",
    "        est, _ = estimate_fourier_coefficient(target, s, num_samples=500, rng=rng)\n",
    "        if abs(est) > 0.05:\n",
    "            estimated_coeffs[s] = est\n",
    "\n",
    "print(f\"\\nEstimated {len(estimated_coeffs)} non-negligible degree-≤2 coefficients\")\n",
    "\n",
    "# Use estimated coefficients for prediction\n",
    "def predict(x):\n",
    "    val = sum(coeff * (1 - 2 * (bin(x & s).count('1') % 2)) \n",
    "              for s, coeff in estimated_coeffs.items())\n",
    "    return 1 if val > 0 else -1\n",
    "\n",
    "# Test accuracy\n",
    "correct = sum(1 for x in range(16) \n",
    "              if predict(x) == (1 - 2 * int(target.evaluate(x))))\n",
    "\n",
    "print(f\"Learning accuracy: {correct}/16 = {correct/16:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. The Goldreich-Levin Algorithm\n",
    "\n",
    "**Problem:** Find all \"heavy\" Fourier coefficients without enumerating all $2^n$ subsets.\n",
    "\n",
    "**Theorem (Goldreich-Levin, 1989):** There exists an algorithm that, given oracle access to $f$, finds all $S$ with $|\\hat{f}(S)| \\geq \\tau$ using $O(n/\\tau^4)$ queries.\n",
    "\n",
    "### Key Idea: Self-Correction via Restrictions\n",
    "\n",
    "For a random subset $T \\subseteq [n]$:\n",
    "$$\\mathbf{E}_{T,b}[\\hat{f|_{T=b}}(S|_{\\bar{T}})] = \\hat{f}(S)$$\n",
    "\n",
    "This allows recursively finding heavy coefficients by restricting variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T02:24:58.578493Z",
     "iopub.status.busy": "2026-02-02T02:24:58.578413Z",
     "iopub.status.idle": "2026-02-02T02:24:58.592864Z",
     "shell.execute_reply": "2026-02-02T02:24:58.592598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goldreich-Levin Algorithm Demo\n",
      "============================================================\n",
      "Target: XOR (Parity) on 4 bits\n",
      "Fourier sparsity: only 1 non-zero coefficient\n",
      "\n",
      "True heavy coefficients (|f̂(S)| > 0.1):\n",
      "  S=[0, 1, 2, 3]: f̂(S) = 1.0000\n",
      "\n",
      "Goldreich-Levin found 1 heavy coefficient(s):\n",
      "  S=[0, 1, 2, 3]: estimated f̂(S) = 1.0000\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate Goldreich-Levin algorithm\n",
    "from boofun.analysis.learning import goldreich_levin\n",
    "\n",
    "# Parity has only one non-zero Fourier coefficient (at the full set)\n",
    "xor = bf.parity(4)\n",
    "\n",
    "print(\"Goldreich-Levin Algorithm Demo\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Target: XOR (Parity) on 4 bits\")\n",
    "print(f\"Fourier sparsity: only 1 non-zero coefficient\")\n",
    "print()\n",
    "\n",
    "# True Fourier coefficients\n",
    "true_coeffs = xor.fourier()\n",
    "\n",
    "print(\"True heavy coefficients (|f̂(S)| > 0.1):\")\n",
    "for s in range(len(true_coeffs)):\n",
    "    if abs(true_coeffs[s]) > 0.1:\n",
    "        # LSB=x₀: bit i of s means variable i is in subset\n",
    "        subset = [i for i in range(4) if (s >> i) & 1]\n",
    "        print(f\"  S={subset}: f̂(S) = {true_coeffs[s]:.4f}\")\n",
    "\n",
    "# Use Goldreich-Levin to find heavy coefficients\n",
    "heavy = goldreich_levin(xor, threshold=0.3)\n",
    "\n",
    "print(f\"\\nGoldreich-Levin found {len(heavy)} heavy coefficient(s):\")\n",
    "for s, coeff in heavy:\n",
    "    subset = [i for i in range(4) if (s >> i) & 1]\n",
    "    print(f\"  S={subset}: estimated f̂(S) = {coeff:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Spectral Concentration**: Functions with bounded decision tree depth have Fourier weight concentrated on low degrees\n",
    "\n",
    "2. **LMN Theorem**: If a function class has spectral concentration at degree $k$, it's PAC-learnable in time $n^{O(k)}$\n",
    "\n",
    "3. **Fourier Coefficient Estimation**: $O(\\log(1/\\delta)/\\varepsilon^2)$ samples suffice to estimate any $\\hat{f}(S)$ within $\\varepsilon$\n",
    "\n",
    "4. **Goldreich-Levin**: Find heavy Fourier coefficients efficiently without enumeration\n",
    "\n",
    "### Corollaries (from lecture notes)\n",
    "\n",
    "- **Depth-d decision trees**: Learnable in time $n^{O(d)}$\n",
    "- **Size-s decision trees**: Learnable in time $n^{O(\\log s)}$\n",
    "- **Linear Threshold Functions**: Learnable in time $n^{O(1/\\varepsilon^2)}$\n",
    "\n",
    "### Open Questions\n",
    "\n",
    "- Can depth-$d$ decision trees be learned in $\\text{poly}(n, 2^d)$ time?\n",
    "- Can $k$-juntas be learned in $\\text{poly}(n)$ time for $k = \\log n$?\n",
    "- Efficient learning of small DNF/CNF formulas?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
